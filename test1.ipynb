{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import roboverse\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3 import DDPG, HerReplayBuffer\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib.common.wrappers import TimeFeatureWrapper\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "\n",
    "model = TQC.load(\"data/tqc1/tqc_model_873000_steps\")\n",
    "env = roboverse.make(\"Widow250PickPlace-v2\",\n",
    "                         gui=True,\n",
    "                         observation_mode=\"pixels\",\n",
    "                         transpose_image=False)\n",
    "model.set_env(env)\n",
    "env = model.get_env()\n",
    "\n",
    "obs = env.reset()\n",
    "print(\"start render\")\n",
    "for i in range(int(1e4)):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    print(rewards)\n",
    "    env.render(\"human\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Oct 14 2023 15:44:17\n",
      "2024-04-24 22:52:39.883927: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/cv2/../../lib64:/usr/lib/wsl/lib\n",
      "2024-04-24 22:52:39.883971: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from roboverse.policies import policies\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "\n",
    "def make_env(env_id: str, rank: int, seed: int = 0):\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "\n",
    "    :param env_id: the environment ID\n",
    "    :param num_env: the number of environments you wish to have in subprocesses\n",
    "    :param seed: the inital seed for RNG\n",
    "    :param rank: index of the subprocess\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = roboverse.make(env_id,\n",
    "                         gui=False,\n",
    "                         observation_mode=\"pixels\",\n",
    "                         transpose_image=False)\n",
    "        #env = TimeFeatureWrapper(env)\n",
    "        #env.reset(seed=seed + rank)\n",
    "        env.reset()\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "\n",
    "def collect_data(env, model, policy, target, num_trajectories=100, num_timesteps=30):\n",
    "    policy_class = policies[policy]\n",
    "    policy = policy_class(env)\n",
    "    num_success = 0\n",
    "    num_saved = 0\n",
    "    accept_trajectory_key = target\n",
    "    noise = 0.1\n",
    "    EPSILON = 0.1\n",
    "\n",
    "    while num_saved < num_trajectories:\n",
    "        num_saved += 1\n",
    "        num_steps = 1e6\n",
    "        rewards = []\n",
    "        env.reset()\n",
    "        policy.reset()\n",
    "        time.sleep(0.1)\n",
    "        success = False\n",
    "        for j in range(num_timesteps):\n",
    "            action, agent_info = policy.get_action()\n",
    "\n",
    "            # In case we need to pad actions by 1 for easier realNVP modelling \n",
    "            env_action_dim = env.action_space.shape[0]\n",
    "            #if env_action_dim - action.shape[0] == 1:\n",
    "            #    action = np.append(action, 0)\n",
    "            action += np.random.normal(scale=noise, size=(env_action_dim,))\n",
    "            action = np.clip(action, -1 + EPSILON, 1 - EPSILON)\n",
    "            observation = env.get_observation()\n",
    "            observation[\"image\"] = np.transpose(observation[\"image\"], (2, 0, 1))\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "            next_observation[\"image\"] = np.transpose(next_observation[\"image\"], (2, 0, 1))\n",
    "            rewards.append(reward)\n",
    "            success = sum(rewards) > 70\n",
    "            model.replay_buffer.add(observation, next_observation, action, reward, np.array([done]), [{}])\n",
    "\n",
    "            if success and num_steps > 1e3: #info[accept_trajectory_key]\n",
    "                num_steps = j\n",
    "\n",
    "            if success and j > 23: #info[accept_trajectory_key]\n",
    "                break\n",
    "            if done or agent_info['done']:\n",
    "                break\n",
    "\n",
    "        if success: #info[accept_trajectory_key]\n",
    "            PRINT = False\n",
    "            if PRINT:\n",
    "                print(\"num_timesteps: \", num_steps, rewards)\n",
    "                #print(observation[\"image\"].shape)\n",
    "                #print(next_observation[\"image\"].shape)\n",
    "            num_success += 1\n",
    "        if num_saved%100 == 0:\n",
    "            print(f\"num_trajectories: {num_saved} success rate: {num_success/num_saved} Reward: {sum(rewards)}\")\n",
    "\n",
    "    print(\"success rate: {}\".format(num_success / (num_saved)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment Widow250PickPlace-v2 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'dict'>`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: int64\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_trajectories: 100 success rate: 0.79 Reward: 133.0\n",
      "num_trajectories: 200 success rate: 0.84 Reward: 81.0\n",
      "num_trajectories: 300 success rate: 0.86 Reward: 80.0\n",
      "num_trajectories: 400 success rate: 0.8725 Reward: 133.0\n",
      "num_trajectories: 500 success rate: 0.88 Reward: 137.0\n",
      "num_trajectories: 600 success rate: 0.88 Reward: 76.0\n",
      "num_trajectories: 700 success rate: 0.8885714285714286 Reward: 81.0\n",
      "num_trajectories: 800 success rate: 0.89 Reward: 84.0\n",
      "num_trajectories: 900 success rate: 0.8944444444444445 Reward: 84.0\n",
      "num_trajectories: 1000 success rate: 0.893 Reward: 130.0\n",
      "num_trajectories: 1100 success rate: 0.8927272727272727 Reward: 80.0\n",
      "num_trajectories: 1200 success rate: 0.895 Reward: 81.0\n",
      "num_trajectories: 1300 success rate: 0.8946153846153846 Reward: 80.0\n",
      "num_trajectories: 1400 success rate: 0.8935714285714286 Reward: 78.0\n",
      "num_trajectories: 1500 success rate: 0.8946666666666667 Reward: 129.0\n",
      "num_trajectories: 1600 success rate: 0.89375 Reward: 82.0\n",
      "num_trajectories: 1700 success rate: 0.8941176470588236 Reward: 80.0\n",
      "num_trajectories: 1800 success rate: 0.8955555555555555 Reward: 80.0\n",
      "num_trajectories: 1900 success rate: 0.8952631578947369 Reward: 80.0\n",
      "num_trajectories: 2000 success rate: 0.895 Reward: 80.0\n",
      "num_trajectories: 2100 success rate: 0.8947619047619048 Reward: -30.0\n",
      "num_trajectories: 2200 success rate: 0.8918181818181818 Reward: 21.0\n",
      "num_trajectories: 2300 success rate: 0.8926086956521739 Reward: 78.0\n",
      "num_trajectories: 2400 success rate: 0.8929166666666667 Reward: 82.0\n",
      "num_trajectories: 2500 success rate: 0.8912 Reward: 125.0\n",
      "num_trajectories: 2600 success rate: 0.8876923076923077 Reward: 76.0\n",
      "num_trajectories: 2700 success rate: 0.8877777777777778 Reward: 80.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m COLLECT\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m COLLECT:\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mcollect_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpickplace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplace_success_target\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     model\u001b[38;5;241m.\u001b[39msave_replay_buffer(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/seed_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tqc_expert_pick_place\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[1], line 57\u001b[0m, in \u001b[0;36mcollect_data\u001b[0;34m(env, model, policy, target, num_trajectories, num_timesteps)\u001b[0m\n\u001b[1;32m     55\u001b[0m observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_observation()\n\u001b[1;32m     56\u001b[0m observation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(observation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m], (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 57\u001b[0m next_observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m next_observation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(next_observation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m], (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     59\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "File \u001b[0;32m~/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/roboverse/roboverse/envs/widow250.py:273\u001b[0m, in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m    270\u001b[0m target_ee_pos \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(target_ee_pos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mee_pos_low,\n\u001b[1;32m    271\u001b[0m                         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mee_pos_high)\n\u001b[1;32m    272\u001b[0m target_gripper_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(target_gripper_state, GRIPPER_LIMITS_LOW,\n\u001b[0;32m--> 273\u001b[0m                                GRIPPER_LIMITS_HIGH)\n\u001b[1;32m    275\u001b[0m bullet\u001b[38;5;241m.\u001b[39mapply_action_ik(\n\u001b[1;32m    276\u001b[0m     target_ee_pos, target_ee_quat, target_gripper_state,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrobot_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m     joint_range\u001b[38;5;241m=\u001b[39mJOINT_RANGE,\n\u001b[1;32m    283\u001b[0m     num_sim_steps\u001b[38;5;241m=\u001b[39mnum_sim_steps)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_neutral_action \u001b[38;5;129;01mand\u001b[39;00m neutral_action \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m:\n",
      "File \u001b[0;32m~/roboverse/roboverse/bullet/control.py:158\u001b[0m, in \u001b[0;36mapply_action_ik\u001b[0;34m(target_ee_pos, target_ee_quat, target_gripper_state, robot_id, end_effector_index, movable_joints, lower_limit, upper_limit, rest_pose, joint_range, num_sim_steps)\u001b[0m\n\u001b[1;32m    150\u001b[0m p\u001b[38;5;241m.\u001b[39msetJointMotorControl2(robot_id,\n\u001b[1;32m    151\u001b[0m                         movable_joints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    152\u001b[0m                         controlMode\u001b[38;5;241m=\u001b[39mp\u001b[38;5;241m.\u001b[39mPOSITION_CONTROL,\n\u001b[1;32m    153\u001b[0m                         targetPosition\u001b[38;5;241m=\u001b[39mtarget_gripper_state[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    154\u001b[0m                         force\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m    155\u001b[0m                         positionGain\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.03\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_sim_steps):\n\u001b[0;32m--> 158\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstepSimulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import roboverse\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3 import DDPG, HerReplayBuffer\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib.common.wrappers import TimeFeatureWrapper\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "\n",
    "env = roboverse.make(\"Widow250PickPlace-v2\",\n",
    "                         gui=False,\n",
    "                         observation_mode=\"pixels\",\n",
    "                         transpose_image=False)\n",
    "#env = TimeFeatureWrapper(env)\n",
    "#env = DummyVecEnv([make_env(\"Widow250PickPlace-v1\", i) for i in range(4)])\n",
    "seed = 1\n",
    "obs = env.reset()\n",
    "\n",
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=1000,\n",
    "  save_path=f\"./data/seed_{seed}/\",\n",
    "  name_prefix=\"tqc_model\",\n",
    "  save_replay_buffer=False,\n",
    "  save_vecnormalize=False,\n",
    ")\n",
    "\n",
    "model = TQC(env=env, batch_size=2048, buffer_size=200_000, gamma=0.95, learning_rate=0.001, policy='MultiInputPolicy',\n",
    "             policy_kwargs=dict(net_arch=[512, 512, 512], n_critics=2),\n",
    "             replay_buffer_class=HerReplayBuffer,\n",
    "             replay_buffer_kwargs=dict(goal_selection_strategy='future', n_sampled_goal=4),\n",
    "             tau=0.05, learning_starts=200, verbose=1)\n",
    "\n",
    "#model = TQC.load(\"data/tqc\")\n",
    "#model.set_env(env)\n",
    "COLLECT=True\n",
    "if COLLECT:\n",
    "    collect_data(env, model, \"pickplace\", \"place_success_target\", 3500, 30)\n",
    "    model.save_replay_buffer(f\"data/seed_{seed}/tqc_expert_pick_place\")\n",
    "else:\n",
    "    print(\"load_replay_buffer\")\n",
    "    model.load_replay_buffer(f\"data/seed_{seed}/tqc_expert_pick_place\")\n",
    "\n",
    "print(\"start pre-training from buffer only\")\n",
    "model.learn(total_timesteps=0, callback=checkpoint_callback, log_interval=5, tb_log_name=\"exp\", reset_num_timesteps = False, progress_bar=True)\n",
    "model.train(gradient_steps=20000)\n",
    "\n",
    "print(\"start learning\")\n",
    "model.learn(total_timesteps=480_000, callback=checkpoint_callback, log_interval=5, tb_log_name=\"exp\", reset_num_timesteps = False, progress_bar=True)\n",
    "model.save(f\"data/seed_{seed}/tqc_pick_place\")\n",
    "model.save_replay_buffer(f\"data/seed_{seed}/tqc_trained_pick_place\")\n",
    "\n",
    "print(\"load_replay_buffer\")\n",
    "model.load_replay_buffer(f\"data/seed_{seed}/tqc_expert_pick_place\")\n",
    "collect_data(env, model, \"pickplace\", \"place_success_target\", 10000, 35)\n",
    "model.save_replay_buffer(f\"data/seed_{seed}/tqc_expert_pick_place\")\n",
    "\n",
    "model.learn(total_timesteps=500_000, callback=checkpoint_callback, log_interval=5, tb_log_name=\"exp\", reset_num_timesteps = False, progress_bar=True)\n",
    "model.save(f\"data/seed_{seed}/tqc_pick_place\")\n",
    "model.save_replay_buffer(f\"data/seed_{seed}/tqc_trained_pick_place\")\n",
    "\n",
    "print(\"finish learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment Widow250PickPlace-v2 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'dict'>`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: int64\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import roboverse\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3 import DDPG, HerReplayBuffer\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib.common.wrappers import TimeFeatureWrapper\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "\n",
    "env = roboverse.make(\"Widow250PickPlace-v2\",\n",
    "                         gui=False,\n",
    "                         observation_mode=\"pixels\",\n",
    "                         transpose_image=False)\n",
    "#env = TimeFeatureWrapper(env)\n",
    "#env = DummyVecEnv([make_env(\"Widow250PickPlace-v1\", i) for i in range(4)])\n",
    "seed = 2\n",
    "obs = env.reset()\n",
    "\n",
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=1000,\n",
    "  save_path=f\"./data/seed_{seed}/\",\n",
    "  name_prefix=\"tqc_model\",\n",
    "  save_replay_buffer=False,\n",
    "  save_vecnormalize=False,\n",
    ")\n",
    "\n",
    "model = TQC(env=env, batch_size=2048, buffer_size=400_000, gamma=0.95, learning_rate=0.001, policy='MultiInputPolicy',\n",
    "             policy_kwargs=dict(net_arch=[512, 512, 512], n_critics=2),\n",
    "             replay_buffer_class=HerReplayBuffer,\n",
    "             replay_buffer_kwargs=dict(goal_selection_strategy='future', n_sampled_goal=4),\n",
    "             tau=0.05, learning_starts=200, verbose=1)\n",
    "\n",
    "#model = TQC.load(\"data/tqc\")\n",
    "#model.set_env(env)\n",
    "COLLECT=True\n",
    "if COLLECT:\n",
    "    collect_data(env, model, \"pickplace\", \"place_success_target\", 10000, 30)\n",
    "    model.save_replay_buffer(f\"data/seed_{seed}/tqc_expert_pick_place\")\n",
    "else:\n",
    "    print(\"load_replay_buffer\")\n",
    "    model.load_replay_buffer(f\"data/seed_{seed}/tqc_expert_pick_place\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
