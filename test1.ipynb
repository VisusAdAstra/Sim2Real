{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import roboverse\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3 import DDPG, HerReplayBuffer\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib.common.wrappers import TimeFeatureWrapper\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "\n",
    "model = TQC.load(\"data/tqc1/tqc_model_873000_steps\")\n",
    "env = roboverse.make(\"Widow250PickPlace-v2\",\n",
    "                         gui=True,\n",
    "                         observation_mode=\"pixels\",\n",
    "                         transpose_image=False)\n",
    "model.set_env(env)\n",
    "env = model.get_env()\n",
    "\n",
    "obs = env.reset()\n",
    "print(\"start render\")\n",
    "for i in range(int(1e4)):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    print(rewards)\n",
    "    env.render(\"human\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Oct 14 2023 15:44:17\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from roboverse.policies import policies\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "\n",
    "def make_env(env_id: str, rank: int, seed: int = 0):\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "\n",
    "    :param env_id: the environment ID\n",
    "    :param num_env: the number of environments you wish to have in subprocesses\n",
    "    :param seed: the inital seed for RNG\n",
    "    :param rank: index of the subprocess\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = roboverse.make(env_id,\n",
    "                         gui=False,\n",
    "                         observation_mode=\"pixels\",\n",
    "                         transpose_image=False)\n",
    "        #env = TimeFeatureWrapper(env)\n",
    "        #env.reset(seed=seed + rank)\n",
    "        env.reset()\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "\n",
    "def collect_data(env, model, policy, target, num_trajectories=100, num_timesteps=30):\n",
    "    policy_class = policies[policy]\n",
    "    policy = policy_class(env)\n",
    "    num_success = 0\n",
    "    num_saved = 0\n",
    "    accept_trajectory_key = target\n",
    "    noise = 0.1\n",
    "    EPSILON = 0.1\n",
    "\n",
    "    while num_saved < num_trajectories:\n",
    "        num_saved += 1\n",
    "        num_steps = 1e6\n",
    "        rewards = []\n",
    "        env.reset()\n",
    "        policy.reset()\n",
    "        time.sleep(0.1)\n",
    "        success = False\n",
    "        for j in range(num_timesteps):\n",
    "            action, agent_info = policy.get_action()\n",
    "\n",
    "            # In case we need to pad actions by 1 for easier realNVP modelling \n",
    "            env_action_dim = env.action_space.shape[0]\n",
    "            #if env_action_dim - action.shape[0] == 1:\n",
    "            #    action = np.append(action, 0)\n",
    "            action += np.random.normal(scale=noise, size=(env_action_dim,))\n",
    "            action = np.clip(action, -1 + EPSILON, 1 - EPSILON)\n",
    "            observation = env.get_observation()\n",
    "            observation[\"image\"] = np.transpose(observation[\"image\"], (2, 0, 1))\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "            next_observation[\"image\"] = np.transpose(next_observation[\"image\"], (2, 0, 1))\n",
    "            rewards.append(reward)\n",
    "            success = sum(rewards) > 70\n",
    "            model.replay_buffer.add(observation, next_observation, action, reward, np.array([done]), [{}])\n",
    "\n",
    "            if success and num_steps > 1e3: #info[accept_trajectory_key]\n",
    "                num_steps = j\n",
    "\n",
    "            if success and j > 23: #info[accept_trajectory_key]\n",
    "                break\n",
    "            if done or agent_info['done']:\n",
    "                break\n",
    "\n",
    "        if success: #info[accept_trajectory_key]\n",
    "            PRINT = False\n",
    "            if PRINT:\n",
    "                print(\"num_timesteps: \", num_steps, rewards)\n",
    "                #print(observation[\"image\"].shape)\n",
    "                #print(next_observation[\"image\"].shape)\n",
    "            num_success += 1\n",
    "        if num_saved%100 == 0:\n",
    "            print(f\"num_trajectories: {num_saved} success rate: {num_success/num_saved} Reward: {sum(rewards)}\")\n",
    "\n",
    "    print(\"success rate: {}\".format(num_success / (num_saved)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment Widow250PickPlace-v2 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'dict'>`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: int64\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_trajectories: 100 success rate: 0.79 Reward: 133.0\n",
      "num_trajectories: 200 success rate: 0.84 Reward: 81.0\n",
      "num_trajectories: 300 success rate: 0.86 Reward: 80.0\n",
      "num_trajectories: 400 success rate: 0.8725 Reward: 133.0\n",
      "num_trajectories: 500 success rate: 0.88 Reward: 137.0\n",
      "num_trajectories: 600 success rate: 0.88 Reward: 76.0\n",
      "num_trajectories: 700 success rate: 0.8885714285714286 Reward: 81.0\n",
      "num_trajectories: 800 success rate: 0.89 Reward: 84.0\n",
      "num_trajectories: 900 success rate: 0.8944444444444445 Reward: 84.0\n",
      "num_trajectories: 1000 success rate: 0.893 Reward: 130.0\n",
      "num_trajectories: 1100 success rate: 0.8927272727272727 Reward: 80.0\n",
      "num_trajectories: 1200 success rate: 0.895 Reward: 81.0\n",
      "num_trajectories: 1300 success rate: 0.8946153846153846 Reward: 80.0\n",
      "num_trajectories: 1400 success rate: 0.8935714285714286 Reward: 78.0\n",
      "num_trajectories: 1500 success rate: 0.8946666666666667 Reward: 129.0\n",
      "num_trajectories: 1600 success rate: 0.89375 Reward: 82.0\n",
      "num_trajectories: 1700 success rate: 0.8941176470588236 Reward: 80.0\n",
      "num_trajectories: 1800 success rate: 0.8955555555555555 Reward: 80.0\n",
      "num_trajectories: 1900 success rate: 0.8952631578947369 Reward: 80.0\n",
      "num_trajectories: 2000 success rate: 0.895 Reward: 80.0\n",
      "num_trajectories: 2100 success rate: 0.8947619047619048 Reward: -30.0\n",
      "num_trajectories: 2200 success rate: 0.8918181818181818 Reward: 21.0\n",
      "num_trajectories: 2300 success rate: 0.8926086956521739 Reward: 78.0\n",
      "num_trajectories: 2400 success rate: 0.8929166666666667 Reward: 82.0\n",
      "num_trajectories: 2500 success rate: 0.8912 Reward: 125.0\n",
      "num_trajectories: 2600 success rate: 0.8876923076923077 Reward: 76.0\n",
      "num_trajectories: 2700 success rate: 0.8877777777777778 Reward: 80.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m COLLECT\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m COLLECT:\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mcollect_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpickplace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplace_success_target\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     model\u001b[38;5;241m.\u001b[39msave_replay_buffer(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/seed_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tqc_expert_pick_place\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[1], line 57\u001b[0m, in \u001b[0;36mcollect_data\u001b[0;34m(env, model, policy, target, num_trajectories, num_timesteps)\u001b[0m\n\u001b[1;32m     55\u001b[0m observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_observation()\n\u001b[1;32m     56\u001b[0m observation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(observation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m], (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 57\u001b[0m next_observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m next_observation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(next_observation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m], (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     59\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "File \u001b[0;32m~/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/roboverse/roboverse/envs/widow250.py:273\u001b[0m, in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m    270\u001b[0m target_ee_pos \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(target_ee_pos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mee_pos_low,\n\u001b[1;32m    271\u001b[0m                         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mee_pos_high)\n\u001b[1;32m    272\u001b[0m target_gripper_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(target_gripper_state, GRIPPER_LIMITS_LOW,\n\u001b[0;32m--> 273\u001b[0m                                GRIPPER_LIMITS_HIGH)\n\u001b[1;32m    275\u001b[0m bullet\u001b[38;5;241m.\u001b[39mapply_action_ik(\n\u001b[1;32m    276\u001b[0m     target_ee_pos, target_ee_quat, target_gripper_state,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrobot_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m     joint_range\u001b[38;5;241m=\u001b[39mJOINT_RANGE,\n\u001b[1;32m    283\u001b[0m     num_sim_steps\u001b[38;5;241m=\u001b[39mnum_sim_steps)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_neutral_action \u001b[38;5;129;01mand\u001b[39;00m neutral_action \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m:\n",
      "File \u001b[0;32m~/roboverse/roboverse/bullet/control.py:158\u001b[0m, in \u001b[0;36mapply_action_ik\u001b[0;34m(target_ee_pos, target_ee_quat, target_gripper_state, robot_id, end_effector_index, movable_joints, lower_limit, upper_limit, rest_pose, joint_range, num_sim_steps)\u001b[0m\n\u001b[1;32m    150\u001b[0m p\u001b[38;5;241m.\u001b[39msetJointMotorControl2(robot_id,\n\u001b[1;32m    151\u001b[0m                         movable_joints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    152\u001b[0m                         controlMode\u001b[38;5;241m=\u001b[39mp\u001b[38;5;241m.\u001b[39mPOSITION_CONTROL,\n\u001b[1;32m    153\u001b[0m                         targetPosition\u001b[38;5;241m=\u001b[39mtarget_gripper_state[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    154\u001b[0m                         force\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m    155\u001b[0m                         positionGain\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.03\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_sim_steps):\n\u001b[0;32m--> 158\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstepSimulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import roboverse\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3 import DDPG, HerReplayBuffer\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib.common.wrappers import TimeFeatureWrapper\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "\n",
    "env = roboverse.make(\"Widow250PickPlace-v2\",\n",
    "                         gui=False,\n",
    "                         observation_mode=\"pixels\",\n",
    "                         transpose_image=False)\n",
    "#env = TimeFeatureWrapper(env)\n",
    "#env = DummyVecEnv([make_env(\"Widow250PickPlace-v1\", i) for i in range(4)])\n",
    "seed = 1\n",
    "obs = env.reset()\n",
    "\n",
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=1000,\n",
    "  save_path=f\"./data/seed_{seed}/\",\n",
    "  name_prefix=\"tqc_model\",\n",
    "  save_replay_buffer=False,\n",
    "  save_vecnormalize=False,\n",
    ")\n",
    "\n",
    "model = TQC(env=env, batch_size=2048, buffer_size=200_000, gamma=0.95, learning_rate=0.001, policy='MultiInputPolicy',\n",
    "             policy_kwargs=dict(net_arch=[512, 512, 512], n_critics=2),\n",
    "             replay_buffer_class=HerReplayBuffer,\n",
    "             replay_buffer_kwargs=dict(goal_selection_strategy='future', n_sampled_goal=4),\n",
    "             tau=0.05, learning_starts=200, verbose=1)\n",
    "\n",
    "#model = TQC.load(\"data/tqc\")\n",
    "#model.set_env(env)\n",
    "COLLECT=True\n",
    "if COLLECT:\n",
    "    collect_data(env, model, \"pickplace\", \"place_success_target\", 3500, 30)\n",
    "    model.save_replay_buffer(f\"data/seed_{seed}/tqc_expert_pick_place\")\n",
    "else:\n",
    "    print(\"load_replay_buffer\")\n",
    "    model.load_replay_buffer(f\"data/seed_{seed}/tqc_expert_pick_place\")\n",
    "\n",
    "print(\"start pre-training from buffer only\")\n",
    "model.learn(total_timesteps=0, callback=checkpoint_callback, log_interval=5, tb_log_name=\"exp\", reset_num_timesteps = False, progress_bar=True)\n",
    "model.train(gradient_steps=20000)\n",
    "\n",
    "print(\"start learning\")\n",
    "model.learn(total_timesteps=480_000, callback=checkpoint_callback, log_interval=5, tb_log_name=\"exp\", reset_num_timesteps = False, progress_bar=True)\n",
    "model.save(f\"data/seed_{seed}/tqc_pick_place\")\n",
    "model.save_replay_buffer(f\"data/seed_{seed}/tqc_trained_pick_place\")\n",
    "\n",
    "print(\"load_replay_buffer\")\n",
    "model.load_replay_buffer(f\"data/seed_{seed}/tqc_expert_pick_place\")\n",
    "collect_data(env, model, \"pickplace\", \"place_success_target\", 10000, 35)\n",
    "model.save_replay_buffer(f\"data/seed_{seed}/tqc_expert_pick_place\")\n",
    "\n",
    "model.learn(total_timesteps=500_000, callback=checkpoint_callback, log_interval=5, tb_log_name=\"exp\", reset_num_timesteps = False, progress_bar=True)\n",
    "model.save(f\"data/seed_{seed}/tqc_pick_place\")\n",
    "model.save_replay_buffer(f\"data/seed_{seed}/tqc_trained_pick_place\")\n",
    "\n",
    "print(\"finish learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment Widow250PickPlace-v2 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'dict'>`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: int64\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_trajectories: 100 success rate: 0.85 Reward: 82.0\n",
      "num_trajectories: 200 success rate: 0.87 Reward: 84.0\n",
      "num_trajectories: 300 success rate: 0.8766666666666667 Reward: 133.0\n",
      "num_trajectories: 400 success rate: 0.8925 Reward: 80.0\n",
      "num_trajectories: 500 success rate: 0.9 Reward: 86.0\n",
      "num_trajectories: 600 success rate: 0.9016666666666666 Reward: 84.0\n",
      "num_trajectories: 700 success rate: 0.9085714285714286 Reward: 75.0\n",
      "num_trajectories: 800 success rate: 0.91 Reward: 90.0\n",
      "num_trajectories: 900 success rate: 0.91 Reward: 135.0\n",
      "num_trajectories: 1000 success rate: 0.9 Reward: 82.0\n",
      "num_trajectories: 1100 success rate: 0.9009090909090909 Reward: 75.0\n",
      "num_trajectories: 1200 success rate: 0.8983333333333333 Reward: -35.0\n",
      "num_trajectories: 1300 success rate: 0.9 Reward: 78.0\n",
      "num_trajectories: 1400 success rate: 0.8985714285714286 Reward: 84.0\n",
      "num_trajectories: 1500 success rate: 0.898 Reward: 82.0\n",
      "num_trajectories: 1600 success rate: 0.9 Reward: 78.0\n",
      "num_trajectories: 1700 success rate: 0.9 Reward: 76.0\n",
      "num_trajectories: 1800 success rate: 0.8983333333333333 Reward: 139.0\n",
      "num_trajectories: 1900 success rate: 0.8973684210526316 Reward: 80.0\n",
      "num_trajectories: 2000 success rate: 0.898 Reward: 86.0\n",
      "num_trajectories: 2100 success rate: 0.8976190476190476 Reward: 80.0\n",
      "num_trajectories: 2200 success rate: 0.8963636363636364 Reward: 72.0\n",
      "num_trajectories: 2300 success rate: 0.8969565217391304 Reward: 132.0\n",
      "num_trajectories: 2400 success rate: 0.8975 Reward: 84.0\n",
      "num_trajectories: 2500 success rate: 0.8976 Reward: 16.0\n",
      "num_trajectories: 2600 success rate: 0.8980769230769231 Reward: 80.0\n",
      "num_trajectories: 2700 success rate: 0.8992592592592593 Reward: 80.0\n",
      "num_trajectories: 2800 success rate: 0.8989285714285714 Reward: 73.0\n",
      "num_trajectories: 2900 success rate: 0.8972413793103449 Reward: 74.0\n",
      "num_trajectories: 3000 success rate: 0.8973333333333333 Reward: 78.0\n",
      "num_trajectories: 3100 success rate: 0.8964516129032258 Reward: 77.0\n",
      "num_trajectories: 3200 success rate: 0.8959375 Reward: 122.0\n",
      "num_trajectories: 3300 success rate: 0.896969696969697 Reward: 82.0\n",
      "num_trajectories: 3400 success rate: 0.8958823529411765 Reward: -35.0\n",
      "num_trajectories: 3500 success rate: 0.8962857142857142 Reward: 79.0\n",
      "num_trajectories: 3600 success rate: 0.8961111111111111 Reward: 80.0\n",
      "num_trajectories: 3700 success rate: 0.8972972972972973 Reward: 82.0\n",
      "num_trajectories: 3800 success rate: 0.8968421052631579 Reward: 126.0\n",
      "num_trajectories: 3900 success rate: 0.8974358974358975 Reward: 86.0\n",
      "num_trajectories: 4000 success rate: 0.8965 Reward: 127.0\n",
      "num_trajectories: 4100 success rate: 0.8970731707317073 Reward: 80.0\n",
      "num_trajectories: 4200 success rate: 0.8971428571428571 Reward: 16.0\n",
      "num_trajectories: 4300 success rate: 0.8969767441860466 Reward: 80.0\n",
      "num_trajectories: 4400 success rate: 0.8977272727272727 Reward: 131.0\n",
      "num_trajectories: 4500 success rate: 0.8984444444444445 Reward: 72.0\n",
      "num_trajectories: 4600 success rate: 0.898695652173913 Reward: 82.0\n",
      "num_trajectories: 4700 success rate: 0.8997872340425532 Reward: 80.0\n",
      "num_trajectories: 4800 success rate: 0.9 Reward: 135.0\n",
      "num_trajectories: 4900 success rate: 0.9 Reward: 86.0\n",
      "num_trajectories: 5000 success rate: 0.8992 Reward: 86.0\n",
      "num_trajectories: 5100 success rate: 0.8992156862745098 Reward: 131.0\n",
      "num_trajectories: 5200 success rate: 0.8994230769230769 Reward: 79.0\n",
      "num_trajectories: 5300 success rate: 0.8986792452830189 Reward: 16.0\n",
      "num_trajectories: 5400 success rate: 0.8994444444444445 Reward: 82.0\n",
      "num_trajectories: 5500 success rate: 0.8996363636363637 Reward: 81.0\n",
      "num_trajectories: 5600 success rate: 0.8994642857142857 Reward: 84.0\n",
      "num_trajectories: 5700 success rate: 0.8998245614035087 Reward: 129.0\n",
      "num_trajectories: 5800 success rate: 0.8994827586206896 Reward: 135.0\n",
      "num_trajectories: 5900 success rate: 0.8994915254237288 Reward: 76.0\n",
      "num_trajectories: 6000 success rate: 0.8995 Reward: 71.0\n",
      "num_trajectories: 6100 success rate: 0.8988524590163934 Reward: 135.0\n",
      "num_trajectories: 6200 success rate: 0.8983870967741936 Reward: 127.0\n",
      "num_trajectories: 6300 success rate: 0.8977777777777778 Reward: 135.0\n",
      "num_trajectories: 6400 success rate: 0.89734375 Reward: 75.0\n",
      "num_trajectories: 6500 success rate: 0.8973846153846153 Reward: 78.0\n",
      "num_trajectories: 6600 success rate: 0.896969696969697 Reward: 77.0\n",
      "num_trajectories: 6700 success rate: 0.8965671641791044 Reward: 80.0\n",
      "num_trajectories: 6800 success rate: 0.8969117647058824 Reward: 80.0\n",
      "num_trajectories: 6900 success rate: 0.8968115942028986 Reward: 78.0\n",
      "num_trajectories: 7000 success rate: 0.897 Reward: 76.0\n",
      "num_trajectories: 7100 success rate: 0.8976056338028169 Reward: 76.0\n",
      "num_trajectories: 7200 success rate: 0.8980555555555556 Reward: 80.0\n",
      "num_trajectories: 7300 success rate: 0.8982191780821918 Reward: 84.0\n",
      "num_trajectories: 7400 success rate: 0.8978378378378379 Reward: 126.0\n",
      "num_trajectories: 7500 success rate: 0.8972 Reward: 86.0\n",
      "num_trajectories: 7600 success rate: 0.8977631578947368 Reward: -35.0\n",
      "num_trajectories: 7700 success rate: 0.8977922077922078 Reward: 75.0\n",
      "num_trajectories: 7800 success rate: 0.8976923076923077 Reward: 126.0\n",
      "num_trajectories: 7900 success rate: 0.8982278481012658 Reward: 76.0\n",
      "num_trajectories: 8000 success rate: 0.898375 Reward: 80.0\n",
      "num_trajectories: 8100 success rate: 0.8979012345679013 Reward: 129.0\n",
      "num_trajectories: 8200 success rate: 0.8975609756097561 Reward: 88.0\n",
      "num_trajectories: 8300 success rate: 0.8973493975903615 Reward: 84.0\n",
      "num_trajectories: 8400 success rate: 0.8975 Reward: 76.0\n",
      "num_trajectories: 8500 success rate: 0.8977647058823529 Reward: 75.0\n",
      "num_trajectories: 8600 success rate: 0.8977906976744187 Reward: 82.0\n",
      "num_trajectories: 8700 success rate: 0.8977011494252873 Reward: 82.0\n",
      "num_trajectories: 8800 success rate: 0.8979545454545454 Reward: 82.0\n",
      "num_trajectories: 8900 success rate: 0.898314606741573 Reward: 132.0\n",
      "num_trajectories: 9000 success rate: 0.8982222222222223 Reward: 80.0\n",
      "num_trajectories: 9100 success rate: 0.8984615384615384 Reward: -35.0\n",
      "num_trajectories: 9200 success rate: 0.898695652173913 Reward: 129.0\n",
      "num_trajectories: 9300 success rate: 0.8987096774193548 Reward: 74.0\n",
      "num_trajectories: 9400 success rate: 0.898936170212766 Reward: 76.0\n",
      "num_trajectories: 9500 success rate: 0.8993684210526316 Reward: 75.0\n",
      "num_trajectories: 9600 success rate: 0.8994791666666667 Reward: 127.0\n",
      "num_trajectories: 9700 success rate: 0.8995876288659794 Reward: 82.0\n",
      "num_trajectories: 9800 success rate: 0.8994897959183673 Reward: 73.0\n",
      "num_trajectories: 9900 success rate: 0.8994949494949495 Reward: 127.0\n",
      "num_trajectories: 10000 success rate: 0.8996 Reward: -35.0\n",
      "success rate: 0.8996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/stable_baselines3/common/save_util.py:283: UserWarning: Path 'data/seed_2' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import roboverse\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3 import DDPG, HerReplayBuffer\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib.common.wrappers import TimeFeatureWrapper\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "\n",
    "env = roboverse.make(\"Widow250PickPlace-v2\",\n",
    "                         gui=False,\n",
    "                         observation_mode=\"pixels\",\n",
    "                         transpose_image=False)\n",
    "#env = TimeFeatureWrapper(env)\n",
    "#env = DummyVecEnv([make_env(\"Widow250PickPlace-v1\", i) for i in range(4)])\n",
    "seed = 2\n",
    "obs = env.reset()\n",
    "\n",
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=1000,\n",
    "  save_path=f\"./data/seed_{seed}/\",\n",
    "  name_prefix=\"tqc_model\",\n",
    "  save_replay_buffer=False,\n",
    "  save_vecnormalize=False,\n",
    ")\n",
    "\n",
    "model = TQC(env=env, batch_size=2048, buffer_size=300_000, gamma=0.95, learning_rate=0.001, policy='MultiInputPolicy',\n",
    "             policy_kwargs=dict(net_arch=[512, 512, 512], n_critics=2),\n",
    "             replay_buffer_class=HerReplayBuffer,\n",
    "             replay_buffer_kwargs=dict(goal_selection_strategy='future', n_sampled_goal=4),\n",
    "             tau=0.05, learning_starts=200, verbose=1)\n",
    "\n",
    "#model = TQC.load(\"data/tqc\")\n",
    "#model.set_env(env)\n",
    "COLLECT=True\n",
    "if COLLECT:\n",
    "    collect_data(env, model, \"pickplace\", \"place_success_target\", 10000, 35)\n",
    "    model.save_replay_buffer(f\"data/seed_{seed}/tqc_expert_pick_place\")\n",
    "else:\n",
    "    print(\"load_replay_buffer\")\n",
    "    model.load_replay_buffer(f\"data/seed_{seed}/tqc_expert_pick_place\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment Widow250PickPlace-v2 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'dict'>`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: int64\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_trajectories: 100 success rate: 0.95 Reward: 78.0\n",
      "num_trajectories: 200 success rate: 0.89 Reward: -35.0\n",
      "num_trajectories: 300 success rate: 0.8933333333333333 Reward: 82.0\n",
      "num_trajectories: 400 success rate: 0.895 Reward: 79.0\n",
      "num_trajectories: 500 success rate: 0.9 Reward: 84.0\n",
      "num_trajectories: 600 success rate: 0.8916666666666667 Reward: 78.0\n",
      "num_trajectories: 700 success rate: 0.8957142857142857 Reward: 82.0\n",
      "num_trajectories: 800 success rate: 0.89125 Reward: 133.0\n",
      "num_trajectories: 900 success rate: 0.8944444444444445 Reward: 86.0\n",
      "num_trajectories: 1000 success rate: 0.899 Reward: 80.0\n",
      "num_trajectories: 1100 success rate: 0.9018181818181819 Reward: 84.0\n",
      "num_trajectories: 1200 success rate: 0.8991666666666667 Reward: 131.0\n",
      "num_trajectories: 1300 success rate: 0.9023076923076923 Reward: 84.0\n",
      "num_trajectories: 1400 success rate: 0.9014285714285715 Reward: 75.0\n",
      "num_trajectories: 1500 success rate: 0.9013333333333333 Reward: 76.0\n",
      "num_trajectories: 1600 success rate: 0.90125 Reward: 74.0\n",
      "num_trajectories: 1700 success rate: 0.9011764705882352 Reward: 76.0\n",
      "num_trajectories: 1800 success rate: 0.9022222222222223 Reward: 76.0\n",
      "num_trajectories: 1900 success rate: 0.9 Reward: -35.0\n",
      "num_trajectories: 2000 success rate: 0.9005 Reward: -35.0\n",
      "num_trajectories: 2100 success rate: 0.8995238095238095 Reward: 80.0\n",
      "num_trajectories: 2200 success rate: 0.8977272727272727 Reward: 16.0\n",
      "num_trajectories: 2300 success rate: 0.8969565217391304 Reward: 82.0\n",
      "num_trajectories: 2400 success rate: 0.895 Reward: 75.0\n",
      "num_trajectories: 2500 success rate: 0.8972 Reward: 86.0\n",
      "num_trajectories: 2600 success rate: 0.8953846153846153 Reward: 16.0\n",
      "num_trajectories: 2700 success rate: 0.8944444444444445 Reward: 82.0\n",
      "num_trajectories: 2800 success rate: 0.8946428571428572 Reward: 86.0\n",
      "num_trajectories: 2900 success rate: 0.8948275862068965 Reward: 73.0\n",
      "num_trajectories: 3000 success rate: 0.8963333333333333 Reward: 74.0\n",
      "num_trajectories: 3100 success rate: 0.8961290322580645 Reward: 78.0\n",
      "num_trajectories: 3200 success rate: 0.8971875 Reward: 84.0\n",
      "num_trajectories: 3300 success rate: 0.8972727272727272 Reward: 78.0\n",
      "num_trajectories: 3400 success rate: 0.8967647058823529 Reward: 73.0\n",
      "num_trajectories: 3500 success rate: 0.8957142857142857 Reward: 80.0\n",
      "num_trajectories: 3600 success rate: 0.8952777777777777 Reward: 78.0\n",
      "num_trajectories: 3700 success rate: 0.8959459459459459 Reward: 78.0\n",
      "num_trajectories: 3800 success rate: 0.8955263157894737 Reward: 82.0\n",
      "num_trajectories: 3900 success rate: 0.8948717948717949 Reward: -35.0\n",
      "num_trajectories: 4000 success rate: 0.89425 Reward: 84.0\n",
      "num_trajectories: 4100 success rate: 0.8936585365853659 Reward: 131.0\n",
      "num_trajectories: 4200 success rate: 0.8935714285714286 Reward: 75.0\n",
      "num_trajectories: 4300 success rate: 0.8946511627906977 Reward: 92.0\n",
      "num_trajectories: 4400 success rate: 0.8934090909090909 Reward: 76.0\n",
      "num_trajectories: 4500 success rate: 0.8937777777777778 Reward: 84.0\n",
      "num_trajectories: 4600 success rate: 0.8941304347826087 Reward: 76.0\n",
      "num_trajectories: 4700 success rate: 0.8946808510638298 Reward: 86.0\n",
      "num_trajectories: 4800 success rate: 0.8952083333333334 Reward: 82.0\n",
      "num_trajectories: 4900 success rate: 0.8965306122448979 Reward: 124.0\n",
      "num_trajectories: 5000 success rate: 0.8962 Reward: 86.0\n",
      "num_trajectories: 5100 success rate: 0.8968627450980392 Reward: 76.0\n",
      "num_trajectories: 5200 success rate: 0.8965384615384615 Reward: 80.0\n",
      "num_trajectories: 5300 success rate: 0.8969811320754717 Reward: 80.0\n",
      "num_trajectories: 5400 success rate: 0.8959259259259259 Reward: 127.0\n",
      "num_trajectories: 5500 success rate: 0.8958181818181818 Reward: 84.0\n",
      "num_trajectories: 5600 success rate: 0.895 Reward: 16.0\n",
      "num_trajectories: 5700 success rate: 0.8952631578947369 Reward: 84.0\n",
      "num_trajectories: 5800 success rate: 0.8943103448275862 Reward: 79.0\n",
      "num_trajectories: 5900 success rate: 0.8940677966101694 Reward: 71.0\n",
      "num_trajectories: 6000 success rate: 0.8935 Reward: 76.0\n",
      "num_trajectories: 6100 success rate: 0.8931147540983606 Reward: 126.0\n",
      "num_trajectories: 6200 success rate: 0.8935483870967742 Reward: 75.0\n",
      "num_trajectories: 6300 success rate: 0.8938095238095238 Reward: 88.0\n",
      "num_trajectories: 6400 success rate: 0.8940625 Reward: -35.0\n",
      "num_trajectories: 6500 success rate: 0.8936923076923077 Reward: 84.0\n",
      "num_trajectories: 6600 success rate: 0.8928787878787878 Reward: 126.0\n",
      "num_trajectories: 6700 success rate: 0.8928358208955224 Reward: 78.0\n",
      "num_trajectories: 6800 success rate: 0.8933823529411765 Reward: 84.0\n",
      "num_trajectories: 6900 success rate: 0.8931884057971015 Reward: 84.0\n",
      "num_trajectories: 7000 success rate: 0.8935714285714286 Reward: 78.0\n",
      "num_trajectories: 7100 success rate: 0.8933802816901408 Reward: 132.0\n",
      "num_trajectories: 7200 success rate: 0.8934722222222222 Reward: 135.0\n",
      "num_trajectories: 7300 success rate: 0.8941095890410959 Reward: 78.0\n",
      "num_trajectories: 7400 success rate: 0.8948648648648648 Reward: 78.0\n",
      "num_trajectories: 7500 success rate: 0.8948 Reward: 76.0\n",
      "num_trajectories: 7600 success rate: 0.895 Reward: 82.0\n",
      "num_trajectories: 7700 success rate: 0.8954545454545455 Reward: 76.0\n",
      "num_trajectories: 7800 success rate: 0.8955128205128206 Reward: 80.0\n",
      "num_trajectories: 7900 success rate: 0.8955696202531646 Reward: 135.0\n",
      "num_trajectories: 8000 success rate: 0.895 Reward: 16.0\n",
      "num_trajectories: 8100 success rate: 0.8950617283950617 Reward: 79.0\n",
      "num_trajectories: 8200 success rate: 0.8948780487804878 Reward: -35.0\n",
      "num_trajectories: 8300 success rate: 0.8951807228915662 Reward: 78.0\n",
      "num_trajectories: 8400 success rate: 0.8951190476190476 Reward: 88.0\n",
      "num_trajectories: 8500 success rate: 0.8950588235294118 Reward: 133.0\n",
      "num_trajectories: 8600 success rate: 0.895 Reward: 73.0\n",
      "num_trajectories: 8700 success rate: 0.8950574712643679 Reward: 127.0\n",
      "num_trajectories: 8800 success rate: 0.8947727272727273 Reward: 76.0\n",
      "num_trajectories: 8900 success rate: 0.8946067415730337 Reward: 128.0\n",
      "num_trajectories: 9000 success rate: 0.8945555555555555 Reward: 88.0\n",
      "num_trajectories: 9100 success rate: 0.8945054945054945 Reward: 127.0\n",
      "num_trajectories: 9200 success rate: 0.8945652173913043 Reward: 76.0\n",
      "num_trajectories: 9300 success rate: 0.8941935483870967 Reward: 81.0\n",
      "num_trajectories: 9400 success rate: 0.8941489361702127 Reward: -35.0\n",
      "num_trajectories: 9500 success rate: 0.8946315789473684 Reward: 80.0\n",
      "num_trajectories: 9600 success rate: 0.8942708333333333 Reward: 82.0\n",
      "num_trajectories: 9700 success rate: 0.8949484536082474 Reward: 131.0\n",
      "num_trajectories: 9800 success rate: 0.8953061224489796 Reward: 80.0\n",
      "num_trajectories: 9900 success rate: 0.8956565656565657 Reward: 16.0\n",
      "num_trajectories: 10000 success rate: 0.8951 Reward: 82.0\n",
      "success rate: 0.8951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse1/lib/python3.8/site-packages/stable_baselines3/common/save_util.py:283: UserWarning: Path 'data/seed_3' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import roboverse\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3 import DDPG, HerReplayBuffer\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib.common.wrappers import TimeFeatureWrapper\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "\n",
    "env = roboverse.make(\"Widow250PickPlace-v2\",\n",
    "                         gui=False,\n",
    "                         observation_mode=\"pixels\",\n",
    "                         transpose_image=False)\n",
    "#env = TimeFeatureWrapper(env)\n",
    "#env = DummyVecEnv([make_env(\"Widow250PickPlace-v1\", i) for i in range(4)])\n",
    "seed = 3\n",
    "obs = env.reset()\n",
    "\n",
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=1000,\n",
    "  save_path=f\"./data/seed_{seed}/\",\n",
    "  name_prefix=\"tqc_model\",\n",
    "  save_replay_buffer=False,\n",
    "  save_vecnormalize=False,\n",
    ")\n",
    "\n",
    "model = TQC(env=env, batch_size=2048, buffer_size=300_000, gamma=0.95, learning_rate=0.001, policy='MultiInputPolicy',\n",
    "             policy_kwargs=dict(net_arch=[512, 512, 512], n_critics=2),\n",
    "             replay_buffer_class=HerReplayBuffer,\n",
    "             replay_buffer_kwargs=dict(goal_selection_strategy='future', n_sampled_goal=4),\n",
    "             tau=0.05, learning_starts=200, verbose=1)\n",
    "\n",
    "#model = TQC.load(\"data/tqc\")\n",
    "#model.set_env(env)\n",
    "COLLECT=True\n",
    "if COLLECT:\n",
    "    collect_data(env, model, \"pickplace\", \"place_success_target\", 10000, 35)\n",
    "    model.save_replay_buffer(f\"data/seed_{seed}/tqc_expert_pick_place\")\n",
    "else:\n",
    "    print(\"load_replay_buffer\")\n",
    "    model.load_replay_buffer(f\"data/seed_{seed}/tqc_expert_pick_place\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
