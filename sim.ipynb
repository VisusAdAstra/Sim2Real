{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/scripted_collect.py -n 100 -t 30 -e Widow250PickPlace-v1 -pl grasp -a grasp_success_target --noise=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/scripted_collect.py -n 100 -t 30 -e Widow250PickPlace-v0 -pl pickplace -a place_success_target --noise=0.1 --gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/scripted_collect.py -n 100 -t 30 -e Widow250PickPlaceMultiObject-v0 -pl pickplace -a place_success_target --noise=0.1 --gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Oct 14 2023 15:44:17\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import roboverse\n",
    "from roboverse.policies import policies\n",
    "\n",
    "\n",
    "def collect_data(env, model, policy, target, num_trajectories=100, num_timesteps=30):\n",
    "    policy_class = policies[policy]\n",
    "    policy = policy_class(env)\n",
    "    num_success = 0\n",
    "    num_saved = 0\n",
    "    num_attempts = 0\n",
    "    accept_trajectory_key = target\n",
    "    noise = 0.1\n",
    "    EPSILON = 0.1\n",
    "\n",
    "    while num_saved < num_trajectories:\n",
    "        num_attempts += 1\n",
    "        num_steps = -1\n",
    "        rewards = []\n",
    "        env.reset()\n",
    "        policy.reset()\n",
    "        for j in range(num_timesteps):\n",
    "            action, agent_info = policy.get_action()\n",
    "\n",
    "            # In case we need to pad actions by 1 for easier realNVP modelling \n",
    "            env_action_dim = env.action_space.shape[0]\n",
    "            #if env_action_dim - action.shape[0] == 1:\n",
    "            #    action = np.append(action, 0)\n",
    "            action += np.random.normal(scale=noise, size=(env_action_dim,))\n",
    "            action = np.clip(action, -1 + EPSILON, 1 - EPSILON)\n",
    "            observation = env.get_observation()\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "            if not info[accept_trajectory_key]:\n",
    "                reward += 0.99**(num_timesteps-j)/10\n",
    "            rewards.append(reward)\n",
    "            model.replay_buffer.add(observation, next_observation, action, reward, done, [{}])\n",
    "\n",
    "            if info[accept_trajectory_key] and num_steps < 0:\n",
    "                num_steps = j\n",
    "\n",
    "            if info[accept_trajectory_key] and j > 20:\n",
    "                break\n",
    "            if done or agent_info['done']:\n",
    "                break\n",
    "\n",
    "        if info[accept_trajectory_key]:\n",
    "            if True:\n",
    "                print(\"num_timesteps: \", num_steps)\n",
    "                #print(traj[\"observations\"])\n",
    "            num_success += 1\n",
    "            num_saved += 1\n",
    "        print(f\"num_trajectories: {num_saved} success rate: {num_success/num_attempts} Reward: {sum(rewards)}\")\n",
    "\n",
    "    print(\"success rate: {}\".format(num_success / (num_attempts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Microsoft Corporation\n",
      "GL_RENDERER=D3D12 (Intel(R) UHD Graphics 630)\n",
      "GL_VERSION=4.1 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "GL_SHADING_LANGUAGE_VERSION=4.10\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.1 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "Vendor = Microsoft Corporation\n",
      "Renderer = D3D12 (Intel(R) UHD Graphics 630)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = Microsoft Corporation\n",
      "ven = Microsoft Corporation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'dict'>`\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: uint8\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_timesteps:  18\n",
      "num_trajectories: 1 success rate: 1.0 Reward: 4.541710358871574\n",
      "num_timesteps:  20\n",
      "num_trajectories: 2 success rate: 1.0 Reward: 3.630348846043188\n",
      "num_timesteps:  18\n",
      "num_trajectories: 3 success rate: 1.0 Reward: 5.452176533445703\n",
      "num_timesteps:  15\n",
      "num_trajectories: 4 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  13\n",
      "num_trajectories: 5 success rate: 1.0 Reward: 10.0221039179569\n",
      "num_timesteps:  14\n",
      "num_trajectories: 6 success rate: 1.0 Reward: 9.106398237295291\n",
      "num_timesteps:  15\n",
      "num_trajectories: 7 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_trajectories: 7 success rate: 0.875 Reward: 2.5769663034560235\n",
      "num_timesteps:  15\n",
      "num_trajectories: 8 success rate: 0.8888888888888888 Reward: 4.293950467991893\n",
      "num_timesteps:  13\n",
      "num_trajectories: 9 success rate: 0.9 Reward: 10.0221039179569\n",
      "num_timesteps:  13\n",
      "num_trajectories: 10 success rate: 0.9090909090909091 Reward: 10.0221039179569\n",
      "num_timesteps:  17\n",
      "num_trajectories: 11 success rate: 0.9166666666666666 Reward: 6.364424431145807\n",
      "num_timesteps:  16\n",
      "num_trajectories: 12 success rate: 0.9230769230769231 Reward: 7.277549849868909\n",
      "num_timesteps:  17\n",
      "num_trajectories: 13 success rate: 0.9285714285714286 Reward: 6.364424431145807\n",
      "num_timesteps:  17\n",
      "num_trajectories: 14 success rate: 0.9333333333333333 Reward: 6.364424431145807\n",
      "num_timesteps:  13\n",
      "num_trajectories: 15 success rate: 0.9375 Reward: 10.0221039179569\n",
      "num_timesteps:  14\n",
      "num_trajectories: 16 success rate: 0.9411764705882353 Reward: 9.106398237295291\n",
      "num_timesteps:  13\n",
      "num_trajectories: 17 success rate: 0.9444444444444444 Reward: 10.0221039179569\n",
      "num_timesteps:  14\n",
      "num_trajectories: 18 success rate: 0.9473684210526315 Reward: 9.106398237295291\n",
      "num_trajectories: 18 success rate: 0.9 Reward: 2.5769663034560235\n",
      "num_timesteps:  15\n",
      "num_trajectories: 19 success rate: 0.9047619047619048 Reward: 8.19154401440478\n",
      "num_timesteps:  15\n",
      "num_trajectories: 20 success rate: 0.9090909090909091 Reward: 8.19154401440478\n",
      "num_timesteps:  15\n",
      "num_trajectories: 21 success rate: 0.9130434782608695 Reward: 8.19154401440478\n",
      "num_timesteps:  13\n",
      "num_trajectories: 22 success rate: 0.9166666666666666 Reward: 10.0221039179569\n",
      "num_timesteps:  12\n",
      "num_trajectories: 23 success rate: 0.92 Reward: 10.93865254181189\n",
      "num_timesteps:  15\n",
      "num_trajectories: 24 success rate: 0.9230769230769231 Reward: 8.19154401440478\n",
      "num_timesteps:  14\n",
      "num_trajectories: 25 success rate: 0.9259259259259259 Reward: 9.106398237295291\n",
      "num_timesteps:  14\n",
      "num_trajectories: 26 success rate: 0.9285714285714286 Reward: 9.106398237295291\n",
      "num_timesteps:  15\n",
      "num_trajectories: 27 success rate: 0.9310344827586207 Reward: 8.19154401440478\n",
      "num_timesteps:  14\n",
      "num_trajectories: 28 success rate: 0.9333333333333333 Reward: 9.106398237295291\n",
      "num_timesteps:  11\n",
      "num_trajectories: 29 success rate: 0.9354838709677419 Reward: 11.856035679428333\n",
      "num_timesteps:  16\n",
      "num_trajectories: 30 success rate: 0.9375 Reward: 7.277549849868909\n",
      "num_timesteps:  14\n",
      "num_trajectories: 31 success rate: 0.9393939393939394 Reward: 9.106398237295291\n",
      "num_timesteps:  13\n",
      "num_trajectories: 32 success rate: 0.9411764705882353 Reward: 10.0221039179569\n",
      "num_timesteps:  21\n",
      "num_trajectories: 33 success rate: 0.9428571428571428 Reward: 2.7207870535440684\n",
      "num_timesteps:  14\n",
      "num_trajectories: 34 success rate: 0.9444444444444444 Reward: 9.106398237295291\n",
      "num_timesteps:  14\n",
      "num_trajectories: 35 success rate: 0.9459459459459459 Reward: 9.106398237295291\n",
      "num_timesteps:  18\n",
      "num_trajectories: 36 success rate: 0.9473684210526315 Reward: 5.452176533445703\n",
      "num_timesteps:  14\n",
      "num_trajectories: 37 success rate: 0.9487179487179487 Reward: 9.106398237295291\n",
      "num_trajectories: 37 success rate: 0.925 Reward: 2.5769663034560235\n",
      "num_trajectories: 37 success rate: 0.9024390243902439 Reward: 2.5769663034560235\n",
      "num_timesteps:  15\n",
      "num_trajectories: 38 success rate: 0.9047619047619048 Reward: 8.19154401440478\n",
      "num_timesteps:  16\n",
      "num_trajectories: 39 success rate: 0.9069767441860465 Reward: 7.277549849868909\n",
      "num_timesteps:  14\n",
      "num_trajectories: 40 success rate: 0.9090909090909091 Reward: 9.106398237295291\n",
      "num_timesteps:  14\n",
      "num_trajectories: 41 success rate: 0.9111111111111111 Reward: 9.106398237295291\n",
      "num_timesteps:  14\n",
      "num_trajectories: 42 success rate: 0.9130434782608695 Reward: 6.375008757393657\n",
      "num_timesteps:  13\n",
      "num_trajectories: 43 success rate: 0.9148936170212766 Reward: 10.0221039179569\n",
      "num_timesteps:  17\n",
      "num_trajectories: 44 success rate: 0.9166666666666666 Reward: 6.364424431145807\n",
      "num_timesteps:  13\n",
      "num_trajectories: 45 success rate: 0.9183673469387755 Reward: 10.0221039179569\n",
      "num_trajectories: 45 success rate: 0.9 Reward: 2.5769663034560235\n",
      "num_timesteps:  15\n",
      "num_trajectories: 46 success rate: 0.9019607843137255 Reward: 8.19154401440478\n",
      "num_timesteps:  14\n",
      "num_trajectories: 47 success rate: 0.9038461538461539 Reward: 9.106398237295291\n",
      "num_timesteps:  14\n",
      "num_trajectories: 48 success rate: 0.9056603773584906 Reward: 9.106398237295291\n",
      "num_timesteps:  15\n",
      "num_trajectories: 49 success rate: 0.9074074074074074 Reward: 8.19154401440478\n",
      "num_timesteps:  15\n",
      "num_trajectories: 50 success rate: 0.9090909090909091 Reward: 8.19154401440478\n",
      "num_timesteps:  14\n",
      "num_trajectories: 51 success rate: 0.9107142857142857 Reward: 9.106398237295291\n",
      "num_trajectories: 51 success rate: 0.8947368421052632 Reward: 3.490091722179125\n",
      "num_timesteps:  15\n",
      "num_trajectories: 52 success rate: 0.896551724137931 Reward: 8.19154401440478\n",
      "num_timesteps:  17\n",
      "num_trajectories: 53 success rate: 0.8983050847457628 Reward: 6.364424431145807\n",
      "num_timesteps:  18\n",
      "num_trajectories: 54 success rate: 0.9 Reward: 5.452176533445703\n",
      "num_timesteps:  14\n",
      "num_trajectories: 55 success rate: 0.9016393442622951 Reward: 9.106398237295291\n",
      "num_timesteps:  15\n",
      "num_trajectories: 56 success rate: 0.9032258064516129 Reward: 8.19154401440478\n",
      "num_timesteps:  14\n",
      "num_trajectories: 57 success rate: 0.9047619047619048 Reward: 9.106398237295291\n",
      "num_timesteps:  15\n",
      "num_trajectories: 58 success rate: 0.90625 Reward: 8.282895739153144\n",
      "num_timesteps:  12\n",
      "num_trajectories: 59 success rate: 0.9076923076923077 Reward: 10.93865254181189\n",
      "num_timesteps:  16\n",
      "num_trajectories: 60 success rate: 0.9090909090909091 Reward: 7.277549849868909\n",
      "num_timesteps:  15\n",
      "num_trajectories: 61 success rate: 0.9104477611940298 Reward: 8.19154401440478\n",
      "num_timesteps:  16\n",
      "num_trajectories: 62 success rate: 0.9117647058823529 Reward: 7.277549849868909\n",
      "num_timesteps:  25\n",
      "num_trajectories: 63 success rate: 0.9130434782608695 Reward: 3.091767797466024\n",
      "num_timesteps:  14\n",
      "num_trajectories: 64 success rate: 0.9142857142857143 Reward: 9.106398237295291\n",
      "num_timesteps:  13\n",
      "num_trajectories: 65 success rate: 0.9154929577464789 Reward: 10.0221039179569\n",
      "num_timesteps:  17\n",
      "num_trajectories: 66 success rate: 0.9166666666666666 Reward: 6.364424431145807\n",
      "num_timesteps:  15\n",
      "num_trajectories: 67 success rate: 0.9178082191780822 Reward: 8.19154401440478\n",
      "num_timesteps:  14\n",
      "num_trajectories: 68 success rate: 0.918918918918919 Reward: 9.106398237295291\n",
      "num_timesteps:  14\n",
      "num_trajectories: 69 success rate: 0.92 Reward: 9.106398237295291\n",
      "num_timesteps:  13\n",
      "num_trajectories: 70 success rate: 0.9210526315789473 Reward: 10.0221039179569\n",
      "num_trajectories: 70 success rate: 0.9090909090909091 Reward: 2.5769663034560235\n",
      "num_timesteps:  15\n",
      "num_trajectories: 71 success rate: 0.9102564102564102 Reward: 8.19154401440478\n",
      "num_timesteps:  14\n",
      "num_trajectories: 72 success rate: 0.9113924050632911 Reward: 4.6409871657188155\n",
      "num_timesteps:  14\n",
      "num_trajectories: 73 success rate: 0.9125 Reward: 9.106398237295291\n",
      "num_timesteps:  17\n",
      "num_trajectories: 74 success rate: 0.9135802469135802 Reward: 6.364424431145807\n",
      "num_timesteps:  15\n",
      "num_trajectories: 75 success rate: 0.9146341463414634 Reward: 7.278418595681678\n",
      "num_timesteps:  19\n",
      "num_trajectories: 76 success rate: 0.9156626506024096 Reward: 3.7226049528665603\n",
      "num_trajectories: 76 success rate: 0.9047619047619048 Reward: 2.5769663034560235\n",
      "num_timesteps:  12\n",
      "num_trajectories: 77 success rate: 0.9058823529411765 Reward: 10.93865254181189\n",
      "num_timesteps:  14\n",
      "num_trajectories: 78 success rate: 0.9069767441860465 Reward: 9.106398237295291\n",
      "num_timesteps:  15\n",
      "num_trajectories: 79 success rate: 0.9080459770114943 Reward: 8.19154401440478\n",
      "num_timesteps:  16\n",
      "num_trajectories: 80 success rate: 0.9090909090909091 Reward: 7.277549849868909\n",
      "num_timesteps:  13\n",
      "num_trajectories: 81 success rate: 0.9101123595505618 Reward: 10.0221039179569\n",
      "num_timesteps:  18\n",
      "num_trajectories: 82 success rate: 0.9111111111111111 Reward: 4.541710358871574\n",
      "num_timesteps:  15\n",
      "num_trajectories: 83 success rate: 0.9120879120879121 Reward: 8.19154401440478\n",
      "num_timesteps:  17\n",
      "num_trajectories: 84 success rate: 0.9130434782608695 Reward: 6.364424431145807\n",
      "num_timesteps:  25\n",
      "num_trajectories: 85 success rate: 0.9139784946236559 Reward: 3.091767797466024\n",
      "num_timesteps:  13\n",
      "num_trajectories: 86 success rate: 0.9148936170212766 Reward: 10.0221039179569\n",
      "num_timesteps:  16\n",
      "num_trajectories: 87 success rate: 0.9157894736842105 Reward: 7.277549849868909\n",
      "num_timesteps:  15\n",
      "num_trajectories: 88 success rate: 0.9166666666666666 Reward: 8.19154401440478\n",
      "num_timesteps:  13\n",
      "num_trajectories: 89 success rate: 0.9175257731958762 Reward: 10.0221039179569\n",
      "num_timesteps:  18\n",
      "num_trajectories: 90 success rate: 0.9183673469387755 Reward: 5.452176533445703\n",
      "num_timesteps:  14\n",
      "num_trajectories: 91 success rate: 0.9191919191919192 Reward: 9.106398237295291\n",
      "num_timesteps:  12\n",
      "num_trajectories: 92 success rate: 0.92 Reward: 10.93865254181189\n",
      "num_timesteps:  15\n",
      "num_trajectories: 93 success rate: 0.9207920792079208 Reward: 8.19154401440478\n",
      "num_timesteps:  16\n",
      "num_trajectories: 94 success rate: 0.9215686274509803 Reward: 7.277549849868909\n",
      "num_trajectories: 94 success rate: 0.912621359223301 Reward: 2.5769663034560235\n",
      "num_timesteps:  16\n",
      "num_trajectories: 95 success rate: 0.9134615384615384 Reward: 7.277549849868909\n",
      "num_timesteps:  16\n",
      "num_trajectories: 96 success rate: 0.9142857142857143 Reward: 6.459339782118153\n",
      "num_timesteps:  15\n",
      "num_trajectories: 97 success rate: 0.9150943396226415 Reward: 8.19154401440478\n",
      "num_timesteps:  16\n",
      "num_trajectories: 98 success rate: 0.9158878504672897 Reward: 7.277549849868909\n",
      "num_timesteps:  18\n",
      "num_trajectories: 99 success rate: 0.9166666666666666 Reward: 5.452176533445703\n",
      "num_timesteps:  14\n",
      "num_trajectories: 100 success rate: 0.9174311926605505 Reward: 9.106398237295291\n",
      "success rate: 0.9174311926605505\n",
      "num_timesteps:  23\n",
      "num_trajectories: 1 success rate: 1.0 Reward: 2.9044132477352247\n",
      "num_timesteps:  14\n",
      "num_trajectories: 2 success rate: 1.0 Reward: 9.106398237295291\n",
      "num_timesteps:  15\n",
      "num_trajectories: 3 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  13\n",
      "num_trajectories: 4 success rate: 1.0 Reward: 10.0221039179569\n",
      "num_timesteps:  13\n",
      "num_trajectories: 5 success rate: 1.0 Reward: 10.0221039179569\n",
      "num_timesteps:  15\n",
      "num_trajectories: 6 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  16\n",
      "num_trajectories: 7 success rate: 1.0 Reward: 7.277549849868909\n",
      "num_timesteps:  18\n",
      "num_trajectories: 8 success rate: 1.0 Reward: 5.452176533445703\n",
      "num_timesteps:  21\n",
      "num_trajectories: 9 success rate: 1.0 Reward: 2.7207870535440684\n",
      "num_timesteps:  17\n",
      "num_trajectories: 10 success rate: 1.0 Reward: 6.364424431145807\n",
      "num_timesteps:  20\n",
      "num_trajectories: 11 success rate: 1.0 Reward: 3.630348846043188\n",
      "num_timesteps:  13\n",
      "num_trajectories: 12 success rate: 1.0 Reward: 10.0221039179569\n",
      "num_timesteps:  17\n",
      "num_trajectories: 13 success rate: 1.0 Reward: 6.364424431145807\n",
      "num_timesteps:  16\n",
      "num_trajectories: 14 success rate: 1.0 Reward: 7.277549849868909\n",
      "num_timesteps:  14\n",
      "num_trajectories: 15 success rate: 1.0 Reward: 9.106398237295291\n",
      "num_timesteps:  17\n",
      "num_trajectories: 16 success rate: 1.0 Reward: 6.364424431145807\n",
      "num_timesteps:  21\n",
      "num_trajectories: 17 success rate: 1.0 Reward: 2.7207870535440684\n",
      "num_timesteps:  19\n",
      "num_trajectories: 18 success rate: 1.0 Reward: 4.540815020617316\n",
      "num_timesteps:  14\n",
      "num_trajectories: 19 success rate: 1.0 Reward: 8.19415033959519\n",
      "num_timesteps:  14\n",
      "num_trajectories: 20 success rate: 1.0 Reward: 9.106398237295291\n",
      "num_timesteps:  16\n",
      "num_trajectories: 21 success rate: 1.0 Reward: 7.277549849868909\n",
      "num_timesteps:  15\n",
      "num_trajectories: 22 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  19\n",
      "num_trajectories: 23 success rate: 1.0 Reward: 4.540815020617316\n",
      "num_timesteps:  15\n",
      "num_trajectories: 24 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  17\n",
      "num_trajectories: 25 success rate: 1.0 Reward: 6.364424431145807\n",
      "num_timesteps:  15\n",
      "num_trajectories: 26 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  15\n",
      "num_trajectories: 27 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  17\n",
      "num_trajectories: 28 success rate: 1.0 Reward: 6.364424431145807\n",
      "num_timesteps:  14\n",
      "num_trajectories: 29 success rate: 1.0 Reward: 9.106398237295291\n",
      "num_timesteps:  14\n",
      "num_trajectories: 30 success rate: 1.0 Reward: 9.106398237295291\n",
      "num_timesteps:  16\n",
      "num_trajectories: 31 success rate: 1.0 Reward: 7.277549849868909\n",
      "num_timesteps:  18\n",
      "num_trajectories: 32 success rate: 1.0 Reward: 5.452176533445703\n",
      "num_timesteps:  14\n",
      "num_trajectories: 33 success rate: 1.0 Reward: 8.19327281857219\n",
      "num_timesteps:  17\n",
      "num_trajectories: 34 success rate: 1.0 Reward: 4.727127320009456\n",
      "num_timesteps:  17\n",
      "num_trajectories: 35 success rate: 1.0 Reward: 6.364424431145807\n",
      "num_timesteps:  15\n",
      "num_trajectories: 36 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  15\n",
      "num_trajectories: 37 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  15\n",
      "num_trajectories: 38 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_trajectories: 38 success rate: 0.9743589743589743 Reward: 2.5769663034560235\n",
      "num_timesteps:  13\n",
      "num_trajectories: 39 success rate: 0.975 Reward: 10.0221039179569\n",
      "num_timesteps:  18\n",
      "num_trajectories: 40 success rate: 0.975609756097561 Reward: 5.452176533445703\n",
      "num_timesteps:  14\n",
      "num_trajectories: 41 success rate: 0.9761904761904762 Reward: 9.106398237295291\n",
      "num_timesteps:  15\n",
      "num_trajectories: 42 success rate: 0.9767441860465116 Reward: 8.19154401440478\n",
      "num_timesteps:  15\n",
      "num_trajectories: 43 success rate: 0.9772727272727273 Reward: 8.19154401440478\n",
      "num_timesteps:  17\n",
      "num_trajectories: 44 success rate: 0.9777777777777777 Reward: 6.364424431145807\n",
      "num_timesteps:  16\n",
      "num_trajectories: 45 success rate: 0.9782608695652174 Reward: 7.277549849868909\n",
      "num_timesteps:  16\n",
      "num_trajectories: 46 success rate: 0.9787234042553191 Reward: 7.277549849868909\n",
      "num_timesteps:  15\n",
      "num_trajectories: 47 success rate: 0.9791666666666666 Reward: 8.19154401440478\n",
      "num_timesteps:  14\n",
      "num_trajectories: 48 success rate: 0.9795918367346939 Reward: 9.106398237295291\n",
      "num_timesteps:  15\n",
      "num_trajectories: 49 success rate: 0.98 Reward: 8.19154401440478\n",
      "num_timesteps:  14\n",
      "num_trajectories: 50 success rate: 0.9803921568627451 Reward: 9.106398237295291\n",
      "num_timesteps:  13\n",
      "num_trajectories: 51 success rate: 0.9807692307692307 Reward: 10.0221039179569\n",
      "num_timesteps:  17\n",
      "num_trajectories: 52 success rate: 0.9811320754716981 Reward: 6.364424431145807\n",
      "num_timesteps:  16\n",
      "num_trajectories: 53 success rate: 0.9814814814814815 Reward: 7.277549849868909\n",
      "num_timesteps:  18\n",
      "num_trajectories: 54 success rate: 0.9818181818181818 Reward: 5.452176533445703\n",
      "num_timesteps:  16\n",
      "num_trajectories: 55 success rate: 0.9821428571428571 Reward: 7.277549849868909\n",
      "num_timesteps:  13\n",
      "num_trajectories: 56 success rate: 0.9824561403508771 Reward: 10.0221039179569\n",
      "num_timesteps:  15\n",
      "num_trajectories: 57 success rate: 0.9827586206896551 Reward: 8.19154401440478\n",
      "num_timesteps:  16\n",
      "num_trajectories: 58 success rate: 0.9830508474576272 Reward: 5.303339619879229\n",
      "num_timesteps:  11\n",
      "num_trajectories: 59 success rate: 0.9833333333333333 Reward: 11.856035679428333\n",
      "num_timesteps:  17\n",
      "num_trajectories: 60 success rate: 0.9836065573770492 Reward: 6.364424431145807\n",
      "num_trajectories: 60 success rate: 0.967741935483871 Reward: 2.5769663034560235\n",
      "num_timesteps:  15\n",
      "num_trajectories: 61 success rate: 0.9682539682539683 Reward: 8.19154401440478\n",
      "num_timesteps:  14\n",
      "num_trajectories: 62 success rate: 0.96875 Reward: 9.106398237295291\n",
      "num_timesteps:  16\n",
      "num_trajectories: 63 success rate: 0.9692307692307692 Reward: 7.277549849868909\n",
      "num_timesteps:  15\n",
      "num_trajectories: 64 success rate: 0.9696969696969697 Reward: 8.19154401440478\n",
      "num_timesteps:  16\n",
      "num_trajectories: 65 success rate: 0.9701492537313433 Reward: 7.277549849868909\n",
      "num_timesteps:  16\n",
      "num_trajectories: 66 success rate: 0.9705882352941176 Reward: 7.277549849868909\n",
      "num_timesteps:  14\n",
      "num_trajectories: 67 success rate: 0.9710144927536232 Reward: 9.106398237295291\n",
      "num_timesteps:  14\n",
      "num_trajectories: 68 success rate: 0.9714285714285714 Reward: 9.106398237295291\n",
      "num_timesteps:  23\n",
      "num_trajectories: 69 success rate: 0.971830985915493 Reward: 2.9044132477352247\n",
      "num_timesteps:  17\n",
      "num_trajectories: 70 success rate: 0.9722222222222222 Reward: 6.364424431145807\n",
      "num_timesteps:  14\n",
      "num_trajectories: 71 success rate: 0.9726027397260274 Reward: 9.106398237295291\n",
      "num_timesteps:  17\n",
      "num_trajectories: 72 success rate: 0.972972972972973 Reward: 6.364424431145807\n",
      "num_timesteps:  14\n",
      "num_trajectories: 73 success rate: 0.9733333333333334 Reward: 9.106398237295291\n",
      "num_timesteps:  16\n",
      "num_trajectories: 74 success rate: 0.9736842105263158 Reward: 7.277549849868909\n",
      "num_timesteps:  11\n",
      "num_trajectories: 75 success rate: 0.974025974025974 Reward: 11.856035679428333\n",
      "num_timesteps:  16\n",
      "num_trajectories: 76 success rate: 0.9743589743589743 Reward: 7.277549849868909\n",
      "num_timesteps:  16\n",
      "num_trajectories: 77 success rate: 0.9746835443037974 Reward: 7.277549849868909\n",
      "num_timesteps:  14\n",
      "num_trajectories: 78 success rate: 0.975 Reward: 9.106398237295291\n",
      "num_timesteps:  15\n",
      "num_trajectories: 79 success rate: 0.9753086419753086 Reward: 7.279296116704677\n",
      "num_timesteps:  15\n",
      "num_trajectories: 80 success rate: 0.975609756097561 Reward: 8.19154401440478\n",
      "num_timesteps:  14\n",
      "num_trajectories: 81 success rate: 0.9759036144578314 Reward: 9.106398237295291\n",
      "num_timesteps:  15\n",
      "num_trajectories: 82 success rate: 0.9761904761904762 Reward: 8.19154401440478\n",
      "num_timesteps:  14\n",
      "num_trajectories: 83 success rate: 0.9764705882352941 Reward: 9.106398237295291\n",
      "num_timesteps:  19\n",
      "num_trajectories: 84 success rate: 0.9767441860465116 Reward: 4.540815020617316\n",
      "num_trajectories: 84 success rate: 0.9655172413793104 Reward: 2.5769663034560235\n",
      "num_timesteps:  13\n",
      "num_trajectories: 85 success rate: 0.9659090909090909 Reward: 10.0221039179569\n",
      "num_timesteps:  13\n",
      "num_trajectories: 86 success rate: 0.9662921348314607 Reward: 10.0221039179569\n",
      "num_timesteps:  22\n",
      "num_trajectories: 87 success rate: 0.9666666666666667 Reward: 2.8121387782924328\n",
      "num_timesteps:  21\n",
      "num_trajectories: 88 success rate: 0.967032967032967 Reward: 2.7207870535440684\n",
      "num_timesteps:  18\n",
      "num_trajectories: 89 success rate: 0.967391304347826 Reward: 5.452176533445703\n",
      "num_timesteps:  14\n",
      "num_trajectories: 90 success rate: 0.967741935483871 Reward: 9.106398237295291\n",
      "num_timesteps:  11\n",
      "num_trajectories: 91 success rate: 0.9680851063829787 Reward: 11.856035679428333\n",
      "num_timesteps:  17\n",
      "num_trajectories: 92 success rate: 0.968421052631579 Reward: 6.364424431145807\n",
      "num_timesteps:  15\n",
      "num_trajectories: 93 success rate: 0.96875 Reward: 8.19154401440478\n",
      "num_trajectories: 93 success rate: 0.9587628865979382 Reward: 2.5769663034560235\n",
      "num_timesteps:  17\n",
      "num_trajectories: 94 success rate: 0.9591836734693877 Reward: 4.820333854800155\n",
      "num_timesteps:  14\n",
      "num_trajectories: 95 success rate: 0.9595959595959596 Reward: 9.106398237295291\n",
      "num_timesteps:  16\n",
      "num_trajectories: 96 success rate: 0.96 Reward: 7.277549849868909\n",
      "num_timesteps:  15\n",
      "num_trajectories: 97 success rate: 0.9603960396039604 Reward: 8.19154401440478\n",
      "num_timesteps:  17\n",
      "num_trajectories: 98 success rate: 0.9607843137254902 Reward: 6.364424431145807\n",
      "num_timesteps:  15\n",
      "num_trajectories: 99 success rate: 0.9611650485436893 Reward: 6.462867772079896\n",
      "num_timesteps:  17\n",
      "num_trajectories: 100 success rate: 0.9615384615384616 Reward: 6.364424431145807\n",
      "success rate: 0.9615384615384616\n",
      "start learning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0018fbd574496586d9147df6a8bd39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    episodes        | 10       |\n",
      "|    fps             | 13       |\n",
      "|    time_elapsed    | 75       |\n",
      "|    total_timesteps | 1010     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.83    |\n",
      "|    critic_loss     | 60.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 909      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "\n",
    "#env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
    "env = roboverse.make(\"Widow250PickPlace-v1\",\n",
    "                         gui=False,\n",
    "                         transpose_image=False)\n",
    "obs = env.reset()\n",
    "\n",
    "# The noise objects for TD3\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "model = TD3(\"MultiInputPolicy\", env, buffer_size=100000, verbose=1, learning_starts=0) #, action_noise=action_noise\n",
    "collect_data(env, model, \"grasp\", \"grasp_success_target\", 100, 30)\n",
    "model.save_replay_buffer(\"data/td3_expert_grasp1\")\n",
    "collect_data(env, model, \"grasp\", \"grasp_success_target\", 100, 30)\n",
    "model.save_replay_buffer(\"data/td3_expert_grasp2\")\n",
    "\n",
    "print(\"start learning\")\n",
    "for i in range(4):\n",
    "    model.replay_buffer.reset()\n",
    "    model.load_replay_buffer(f\"data/td3_expert_grasp{i%2+1}\")\n",
    "    model.learn(total_timesteps=2000, log_interval=10, progress_bar=True)\n",
    "print(\"finish learning\")\n",
    "model.save(\"data/td3\")\n",
    "vec_env = model.get_env()\n",
    "\n",
    "#del model # remove to demonstrate saving and loading\n",
    "#model = TD3.load(\"data/td3\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "print(\"start render\")\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roboverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
