{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python scripts/scripted_collect.py -n 100 -t 30 -e Widow250PickPlace-v1 -pl grasp -a grasp_success_target --noise=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python scripts/scripted_collect.py -n 100 -t 30 -e Widow250PickPlace-v0 -pl pickplace -a place_success_target --noise=0.1 --gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python scripts/scripted_collect.py -n 100 -t 30 -e Widow250PickPlaceMultiObject-v0 -pl pickplace -a place_success_target --noise=0.1 --gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import roboverse\n",
    "from roboverse.policies import policies\n",
    "\n",
    "\n",
    "def collect_data(env, model, policy, target, num_trajectories=100, num_timesteps=30):\n",
    "    policy_class = policies[policy]\n",
    "    policy = policy_class(env)\n",
    "    num_success = 0\n",
    "    num_saved = 0\n",
    "    num_attempts = 0\n",
    "    accept_trajectory_key = target\n",
    "    noise = 0.1\n",
    "    EPSILON = 0.1\n",
    "\n",
    "    while num_saved < num_trajectories:\n",
    "        num_attempts += 1\n",
    "        num_steps = -1\n",
    "        rewards = []\n",
    "        env.reset()\n",
    "        policy.reset()\n",
    "        for j in range(num_timesteps):\n",
    "            action, agent_info = policy.get_action()\n",
    "\n",
    "            # In case we need to pad actions by 1 for easier realNVP modelling \n",
    "            env_action_dim = env.action_space.shape[0]\n",
    "            #if env_action_dim - action.shape[0] == 1:\n",
    "            #    action = np.append(action, 0)\n",
    "            action += np.random.normal(scale=noise, size=(env_action_dim,))\n",
    "            action = np.clip(action, -1 + EPSILON, 1 - EPSILON)\n",
    "            observation = env.get_observation_stacked() #env.get_observation()\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "            if not info[accept_trajectory_key]:\n",
    "                reward += 0.99**(num_timesteps-j)/10\n",
    "            rewards.append(reward)\n",
    "            model.replay_buffer.add(observation, next_observation, action, reward, done, [{}])\n",
    "\n",
    "            if info[accept_trajectory_key] and num_steps < 0:\n",
    "                num_steps = j\n",
    "\n",
    "            if info[accept_trajectory_key] and j > 20:\n",
    "                break\n",
    "            if done or agent_info['done']:\n",
    "                break\n",
    "\n",
    "        if info[accept_trajectory_key]:\n",
    "            if True:\n",
    "                print(\"num_timesteps: \", num_steps)\n",
    "                #print(traj[\"observations\"])\n",
    "            num_success += 1\n",
    "            num_saved += 1\n",
    "        print(f\"num_trajectories: {num_saved} success rate: {num_success/num_attempts} Reward: {sum(rewards)}\")\n",
    "\n",
    "    print(\"success rate: {}\".format(num_success / (num_attempts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'dict'>`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Widow250PickPlaceEnv' object has no attribute 'get_observation_stacked'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m COLLECT:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m---> 23\u001b[0m         \u001b[43mcollect_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrasp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrasp_success_target\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m         model\u001b[38;5;241m.\u001b[39msave_replay_buffer(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/td3_expert_grasp\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m COLLECT:\n",
      "Cell \u001b[0;32mIn[5], line 31\u001b[0m, in \u001b[0;36mcollect_data\u001b[0;34m(env, model, policy, target, num_trajectories, num_timesteps)\u001b[0m\n\u001b[1;32m     29\u001b[0m action \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(scale\u001b[38;5;241m=\u001b[39mnoise, size\u001b[38;5;241m=\u001b[39m(env_action_dim,))\n\u001b[1;32m     30\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(action, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m EPSILON, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m EPSILON)\n\u001b[0;32m---> 31\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_observation_stacked\u001b[49m() \u001b[38;5;66;03m#env.get_observation()\u001b[39;00m\n\u001b[1;32m     32\u001b[0m next_observation, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m info[accept_trajectory_key]:\n",
      "File \u001b[0;32m~/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Widow250PickPlaceEnv' object has no attribute 'get_observation_stacked'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "\n",
    "COLLECT = True\n",
    "#env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
    "env = roboverse.make(\"Widow250PickPlace-v1\",\n",
    "                         gui=False,\n",
    "                         transpose_image=False)\n",
    "obs = env.reset()\n",
    "\n",
    "# The noise objects for TD3\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "model = TD3(\"MultiInputPolicy\", env, buffer_size=20000, action_noise=action_noise, \\\n",
    "            tensorboard_log=\"data/td3\", verbose=1, learning_starts=0) #noise Required for deterministic policy\n",
    "if COLLECT:\n",
    "    for i in range(2):\n",
    "        collect_data(env, model, \"grasp\", \"grasp_success_target\", 250, 30)\n",
    "        model.save_replay_buffer(f\"data/td3_expert_grasp{i+1}\")\n",
    "\n",
    "if not COLLECT:\n",
    "    model.replay_buffer.reset()\n",
    "    model.load_replay_buffer(f\"data/td3_expert_grasp1\")\n",
    "    model.learn(total_timesteps=0, log_interval=5, tb_log_name=\"exp\", progress_bar=True)\n",
    "    \n",
    "    print(\"start pre-training from buffer only\")\n",
    "    for i in range(2):\n",
    "        model.replay_buffer.reset()\n",
    "        model.load_replay_buffer(f\"data/td3_expert_grasp{i%2+1}\")\n",
    "        model.train(gradient_steps=2500, batch_size=256)\n",
    "\n",
    "    print(\"start learning\")\n",
    "    for i in range(20):\n",
    "        model.replay_buffer.reset()\n",
    "        model.load_replay_buffer(f\"data/td3_expert_grasp1\")\n",
    "        model.load_replay_buffer(f\"data/td3_expert_grasp2\")\n",
    "        model.learn(total_timesteps=2005, log_interval=5, tb_log_name=\"exp\", progress_bar=True)\n",
    "\n",
    "    print(\"finish learning\")\n",
    "    model.save(\"data/td3_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Microsoft Corporation\n",
      "GL_RENDERER=D3D12 (Intel(R) UHD Graphics 630)\n",
      "GL_VERSION=4.1 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "GL_SHADING_LANGUAGE_VERSION=4.10\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.1 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "Vendor = Microsoft Corporation\n",
      "Renderer = D3D12 (Intel(R) UHD Graphics 630)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = Microsoft Corporation\n",
      "ven = Microsoft Corporation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'dict'>`\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "start render\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:225: UserWarning: You tried to render a VecEnv with mode='human' but the render mode defined when initializing the environment must be 'human' or 'rgb_array', not 'None'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# start env with gui\n",
    "env.close()\n",
    "env = roboverse.make(\"Widow250PickPlace-v1\",\n",
    "                         gui=True,\n",
    "                         transpose_image=False)\n",
    "obs = env.reset()\n",
    "model.set_env(env)\n",
    "vec_env = model.get_env()\n",
    "\n",
    "#del model # remove to demonstrate saving and loading\n",
    "#model = TD3.load(\"data/td3_1\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "print(\"start render\")\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_timesteps:  16\n",
      "num_trajectories: 1 success rate: 1.0 Reward: 5.5452921640887824\n",
      "num_timesteps:  17\n",
      "num_trajectories: 2 success rate: 1.0 Reward: 6.364424431145807\n",
      "num_timesteps:  13\n",
      "num_trajectories: 3 success rate: 1.0 Reward: 10.0221039179569\n",
      "num_timesteps:  15\n",
      "num_trajectories: 4 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  16\n",
      "num_trajectories: 5 success rate: 1.0 Reward: 7.277549849868909\n",
      "num_timesteps:  17\n",
      "num_trajectories: 6 success rate: 1.0 Reward: 6.364424431145807\n",
      "num_timesteps:  15\n",
      "num_trajectories: 7 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  15\n",
      "num_trajectories: 8 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  15\n",
      "num_trajectories: 9 success rate: 1.0 Reward: 5.454809185153187\n",
      "num_timesteps:  15\n",
      "num_trajectories: 10 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  16\n",
      "num_trajectories: 11 success rate: 1.0 Reward: 7.277549849868909\n",
      "num_timesteps:  14\n",
      "num_trajectories: 12 success rate: 1.0 Reward: 9.106398237295291\n",
      "num_timesteps:  16\n",
      "num_trajectories: 13 success rate: 1.0 Reward: 7.277549849868909\n",
      "num_timesteps:  14\n",
      "num_trajectories: 14 success rate: 1.0 Reward: 9.106398237295291\n",
      "num_timesteps:  16\n",
      "num_trajectories: 15 success rate: 1.0 Reward: 7.277549849868909\n",
      "num_timesteps:  17\n",
      "num_trajectories: 16 success rate: 1.0 Reward: 6.364424431145807\n",
      "num_timesteps:  13\n",
      "num_trajectories: 17 success rate: 1.0 Reward: 8.385702145074807\n",
      "num_timesteps:  16\n",
      "num_trajectories: 18 success rate: 1.0 Reward: 7.277549849868909\n",
      "num_timesteps:  14\n",
      "num_trajectories: 19 success rate: 1.0 Reward: 9.106398237295291\n",
      "num_timesteps:  13\n",
      "num_trajectories: 20 success rate: 1.0 Reward: 10.0221039179569\n",
      "num_timesteps:  14\n",
      "num_trajectories: 21 success rate: 1.0 Reward: 9.106398237295291\n",
      "num_timesteps:  20\n",
      "num_trajectories: 22 success rate: 1.0 Reward: 3.721700570791552\n",
      "num_timesteps:  15\n",
      "num_trajectories: 23 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  12\n",
      "num_trajectories: 24 success rate: 1.0 Reward: 10.93865254181189\n",
      "num_timesteps:  15\n",
      "num_trajectories: 25 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  18\n",
      "num_trajectories: 26 success rate: 1.0 Reward: 5.452176533445703\n",
      "num_timesteps:  19\n",
      "num_trajectories: 27 success rate: 1.0 Reward: 4.540815020617316\n",
      "num_timesteps:  12\n",
      "num_trajectories: 28 success rate: 1.0 Reward: 10.93865254181189\n",
      "num_trajectories: 28 success rate: 0.9655172413793104 Reward: 2.5769663034560235\n",
      "num_timesteps:  15\n",
      "num_trajectories: 29 success rate: 0.9666666666666667 Reward: 8.19154401440478\n",
      "num_timesteps:  15\n",
      "num_trajectories: 30 success rate: 0.967741935483871 Reward: 8.19154401440478\n",
      "num_timesteps:  17\n",
      "num_trajectories: 31 success rate: 0.96875 Reward: 6.364424431145807\n",
      "num_timesteps:  16\n",
      "num_trajectories: 32 success rate: 0.9696969696969697 Reward: 7.277549849868909\n",
      "num_timesteps:  15\n",
      "num_trajectories: 33 success rate: 0.9705882352941176 Reward: 8.19154401440478\n",
      "num_timesteps:  17\n",
      "num_trajectories: 34 success rate: 0.9714285714285714 Reward: 6.364424431145807\n",
      "num_timesteps:  19\n",
      "num_trajectories: 35 success rate: 0.9722222222222222 Reward: 4.540815020617316\n",
      "num_timesteps:  16\n",
      "num_trajectories: 36 success rate: 0.972972972972973 Reward: 7.277549849868909\n",
      "num_timesteps:  16\n",
      "num_trajectories: 37 success rate: 0.9736842105263158 Reward: 7.277549849868909\n",
      "num_timesteps:  14\n",
      "num_trajectories: 38 success rate: 0.9743589743589743 Reward: 9.106398237295291\n",
      "num_timesteps:  18\n",
      "num_trajectories: 39 success rate: 0.975 Reward: 5.452176533445703\n",
      "num_timesteps:  15\n",
      "num_trajectories: 40 success rate: 0.975609756097561 Reward: 8.19154401440478\n",
      "num_timesteps:  16\n",
      "num_trajectories: 41 success rate: 0.9761904761904762 Reward: 7.277549849868909\n",
      "num_timesteps:  18\n",
      "num_trajectories: 42 success rate: 0.9767441860465116 Reward: 5.452176533445703\n",
      "num_timesteps:  14\n",
      "num_trajectories: 43 success rate: 0.9772727272727273 Reward: 9.106398237295291\n",
      "num_timesteps:  14\n",
      "num_trajectories: 44 success rate: 0.9777777777777777 Reward: 9.106398237295291\n",
      "num_timesteps:  14\n",
      "num_trajectories: 45 success rate: 0.9782608695652174 Reward: 9.106398237295291\n",
      "num_timesteps:  14\n",
      "num_trajectories: 46 success rate: 0.9787234042553191 Reward: 9.106398237295291\n",
      "num_timesteps:  16\n",
      "num_trajectories: 47 success rate: 0.9791666666666666 Reward: 7.6485305937908645\n",
      "num_timesteps:  20\n",
      "num_trajectories: 48 success rate: 0.9795918367346939 Reward: 3.630348846043188\n",
      "num_timesteps:  16\n",
      "num_trajectories: 49 success rate: 0.98 Reward: 7.277549849868909\n",
      "num_timesteps:  15\n",
      "num_trajectories: 50 success rate: 0.9803921568627451 Reward: 6.366170697981574\n",
      "success rate: 0.9803921568627451\n"
     ]
    }
   ],
   "source": [
    "collect_data(env, model, \"grasp\", \"grasp_success_target\", 50, 30)\n",
    "#model.save_replay_buffer(f\"data/td3_expert_grasp_test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roboverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
