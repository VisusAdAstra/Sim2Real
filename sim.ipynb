{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python scripts/scripted_collect.py -n 100 -t 30 -e Widow250PickPlace-v1 -pl grasp -a grasp_success_target --noise=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python scripts/scripted_collect.py -n 100 -t 30 -e Widow250PickPlace-v0 -pl pickplace -a place_success_target --noise=0.1 --gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python scripts/scripted_collect.py -n 100 -t 30 -e Widow250PickPlaceMultiObject-v0 -pl pickplace -a place_success_target --noise=0.1 --gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Oct 14 2023 15:44:17\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import roboverse\n",
    "from roboverse.policies import policies\n",
    "\n",
    "\n",
    "def collect_data(env, model, policy, target, num_trajectories=100, num_timesteps=30):\n",
    "    policy_class = policies[policy]\n",
    "    policy = policy_class(env)\n",
    "    num_success = 0\n",
    "    num_saved = 0\n",
    "    num_attempts = 0\n",
    "    accept_trajectory_key = target\n",
    "    noise = 0.1\n",
    "    EPSILON = 0.1\n",
    "\n",
    "    while num_saved < num_trajectories:\n",
    "        num_attempts += 1\n",
    "        num_steps = -1\n",
    "        rewards = []\n",
    "        env.reset()\n",
    "        policy.reset()\n",
    "        for j in range(num_timesteps):\n",
    "            action, agent_info = policy.get_action()\n",
    "\n",
    "            # In case we need to pad actions by 1 for easier realNVP modelling \n",
    "            env_action_dim = env.action_space.shape[0]\n",
    "            #if env_action_dim - action.shape[0] == 1:\n",
    "            #    action = np.append(action, 0)\n",
    "            action += np.random.normal(scale=noise, size=(env_action_dim,))\n",
    "            action = np.clip(action, -1 + EPSILON, 1 - EPSILON)\n",
    "            observation = env.get_observation_stacked() #env.get_observation()\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "            if not info[accept_trajectory_key]:\n",
    "                reward += 0.99**(num_timesteps-j)/10\n",
    "            rewards.append(reward)\n",
    "            model.replay_buffer.add(observation, next_observation, action, reward, done, [{}])\n",
    "\n",
    "            if info[accept_trajectory_key] and num_steps < 0:\n",
    "                num_steps = j\n",
    "\n",
    "            if info[accept_trajectory_key] and j > 20:\n",
    "                break\n",
    "            if done or agent_info['done']:\n",
    "                break\n",
    "\n",
    "        if info[accept_trajectory_key]:\n",
    "            if True:\n",
    "                print(\"num_timesteps: \", num_steps)\n",
    "                #print(traj[\"observations\"])\n",
    "            num_success += 1\n",
    "            num_saved += 1\n",
    "        print(f\"num_trajectories: {num_saved} success rate: {num_success/num_attempts} Reward: {sum(rewards)}\")\n",
    "\n",
    "    print(\"success rate: {}\".format(num_success / (num_attempts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (4, 6912)\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'dict'>`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to data/td3/exp_3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d9a35117c949d19d3bffc1f06ea141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start pre-training from buffer only\n",
      "start learning\n",
      "Logging to data/td3/exp_4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d83ea43c5a4a5f98ad041f269f6a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:219: \n",
       "DeprecationWarning: <span style=\"color: #808000; text-decoration-color: #808000\">WARN: Core environment is written in old step API which returns one bool instead of two. It is </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">recommended to rewrite the environment with new step API. </span>\n",
       "  logger.deprecation(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:219: \n",
       "DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is \u001b[0m\n",
       "\u001b[33mrecommended to rewrite the environment with new step API. \u001b[0m\n",
       "  logger.deprecation(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:225: \n",
       "DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
       "  if not isinstance(done, (bool, np.bool8)):\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:225: \n",
       "DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
       "  if not isinstance(done, (bool, np.bool8)):\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: \n",
       "UserWarning: <span style=\"color: #808000; text-decoration-color: #808000\">WARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">type: uint8</span>\n",
       "  logger.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: \n",
       "UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual \u001b[0m\n",
       "\u001b[33mtype: uint8\u001b[0m\n",
       "  logger.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:165: \n",
       "UserWarning: <span style=\"color: #808000; text-decoration-color: #808000\">WARN: The obs returned by the `step()` method is not within the observation space.</span>\n",
       "  logger.warn(f\"{pre} is not within the observation space.\")\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:165: \n",
       "UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
       "  logger.warn(f\"{pre} is not within the observation space.\")\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: \n",
       "UserWarning: <span style=\"color: #808000; text-decoration-color: #808000\">WARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">type: float64</span>\n",
       "  logger.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: \n",
       "UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual \u001b[0m\n",
       "\u001b[33mtype: float64\u001b[0m\n",
       "  logger.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    episodes        | 5        |\n",
      "|    fps             | 31       |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 505      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 75.6     |\n",
      "|    critic_loss     | 5.34e+03 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5404     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | 0        |\n",
      "| time/              |          |\n",
      "|    episodes        | 10       |\n",
      "|    fps             | 30       |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 1010     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 98.6     |\n",
      "|    critic_loss     | 6.87e+03 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5909     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "\n",
    "COLLECT = False\n",
    "#env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
    "env = roboverse.make(\"Widow250PickPlace-v1\",\n",
    "                         gui=False,\n",
    "                         transpose_image=False)\n",
    "obs = env.reset()\n",
    "\n",
    "# The noise objects for TD3\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "model = TD3(\"MultiInputPolicy\", env, buffer_size=10000, action_noise=action_noise, \\\n",
    "            tensorboard_log=\"data/td3\", verbose=1, learning_starts=0) #noise Required for deterministic policy\n",
    "if COLLECT:\n",
    "    for i in range(2):\n",
    "        collect_data(env, model, \"grasp\", \"grasp_success_target\", 250, 30)\n",
    "        model.save_replay_buffer(f\"data/td3_expert_grasp{i+1}\")\n",
    "\n",
    "if not COLLECT:\n",
    "    model.replay_buffer.reset()\n",
    "    model.load_replay_buffer(f\"data/td3_expert_grasp1\")\n",
    "    model.learn(total_timesteps=0, log_interval=5, tb_log_name=\"exp\", progress_bar=True)\n",
    "    \n",
    "    print(\"start pre-training from buffer only\")\n",
    "    for i in range(2):\n",
    "        model.replay_buffer.reset()\n",
    "        model.load_replay_buffer(f\"data/td3_expert_grasp{i%2+1}\")\n",
    "        model.train(gradient_steps=2500, batch_size=256)\n",
    "\n",
    "    print(\"start learning\")\n",
    "    for i in range(20):\n",
    "        model.replay_buffer.reset()\n",
    "        model.load_replay_buffer(f\"data/td3_expert_grasp1\")\n",
    "        model.load_replay_buffer(f\"data/td3_expert_grasp2\")\n",
    "        model.learn(total_timesteps=2005, log_interval=5, tb_log_name=\"exp\", progress_bar=True)\n",
    "\n",
    "    print(\"finish learning\")\n",
    "    model.save(\"data/td3_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Microsoft Corporation\n",
      "GL_RENDERER=D3D12 (Intel(R) UHD Graphics 630)\n",
      "GL_VERSION=4.1 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "GL_SHADING_LANGUAGE_VERSION=4.10\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.1 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "Vendor = Microsoft Corporation\n",
      "Renderer = D3D12 (Intel(R) UHD Graphics 630)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = Microsoft Corporation\n",
      "ven = Microsoft Corporation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'dict'>`\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "start render\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:225: UserWarning: You tried to render a VecEnv with mode='human' but the render mode defined when initializing the environment must be 'human' or 'rgb_array', not 'None'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# start env with gui\n",
    "env.close()\n",
    "env = roboverse.make(\"Widow250PickPlace-v1\",\n",
    "                         gui=True,\n",
    "                         transpose_image=False)\n",
    "obs = env.reset()\n",
    "model.set_env(env)\n",
    "vec_env = model.get_env()\n",
    "\n",
    "#del model # remove to demonstrate saving and loading\n",
    "#model = TD3.load(\"data/td3_1\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "print(\"start render\")\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_timesteps:  16\n",
      "num_trajectories: 1 success rate: 1.0 Reward: 5.5452921640887824\n",
      "num_timesteps:  17\n",
      "num_trajectories: 2 success rate: 1.0 Reward: 6.364424431145807\n",
      "num_timesteps:  13\n",
      "num_trajectories: 3 success rate: 1.0 Reward: 10.0221039179569\n",
      "num_timesteps:  15\n",
      "num_trajectories: 4 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  16\n",
      "num_trajectories: 5 success rate: 1.0 Reward: 7.277549849868909\n",
      "num_timesteps:  17\n",
      "num_trajectories: 6 success rate: 1.0 Reward: 6.364424431145807\n",
      "num_timesteps:  15\n",
      "num_trajectories: 7 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  15\n",
      "num_trajectories: 8 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  15\n",
      "num_trajectories: 9 success rate: 1.0 Reward: 5.454809185153187\n",
      "num_timesteps:  15\n",
      "num_trajectories: 10 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  16\n",
      "num_trajectories: 11 success rate: 1.0 Reward: 7.277549849868909\n",
      "num_timesteps:  14\n",
      "num_trajectories: 12 success rate: 1.0 Reward: 9.106398237295291\n",
      "num_timesteps:  16\n",
      "num_trajectories: 13 success rate: 1.0 Reward: 7.277549849868909\n",
      "num_timesteps:  14\n",
      "num_trajectories: 14 success rate: 1.0 Reward: 9.106398237295291\n",
      "num_timesteps:  16\n",
      "num_trajectories: 15 success rate: 1.0 Reward: 7.277549849868909\n",
      "num_timesteps:  17\n",
      "num_trajectories: 16 success rate: 1.0 Reward: 6.364424431145807\n",
      "num_timesteps:  13\n",
      "num_trajectories: 17 success rate: 1.0 Reward: 8.385702145074807\n",
      "num_timesteps:  16\n",
      "num_trajectories: 18 success rate: 1.0 Reward: 7.277549849868909\n",
      "num_timesteps:  14\n",
      "num_trajectories: 19 success rate: 1.0 Reward: 9.106398237295291\n",
      "num_timesteps:  13\n",
      "num_trajectories: 20 success rate: 1.0 Reward: 10.0221039179569\n",
      "num_timesteps:  14\n",
      "num_trajectories: 21 success rate: 1.0 Reward: 9.106398237295291\n",
      "num_timesteps:  20\n",
      "num_trajectories: 22 success rate: 1.0 Reward: 3.721700570791552\n",
      "num_timesteps:  15\n",
      "num_trajectories: 23 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  12\n",
      "num_trajectories: 24 success rate: 1.0 Reward: 10.93865254181189\n",
      "num_timesteps:  15\n",
      "num_trajectories: 25 success rate: 1.0 Reward: 8.19154401440478\n",
      "num_timesteps:  18\n",
      "num_trajectories: 26 success rate: 1.0 Reward: 5.452176533445703\n",
      "num_timesteps:  19\n",
      "num_trajectories: 27 success rate: 1.0 Reward: 4.540815020617316\n",
      "num_timesteps:  12\n",
      "num_trajectories: 28 success rate: 1.0 Reward: 10.93865254181189\n",
      "num_trajectories: 28 success rate: 0.9655172413793104 Reward: 2.5769663034560235\n",
      "num_timesteps:  15\n",
      "num_trajectories: 29 success rate: 0.9666666666666667 Reward: 8.19154401440478\n",
      "num_timesteps:  15\n",
      "num_trajectories: 30 success rate: 0.967741935483871 Reward: 8.19154401440478\n",
      "num_timesteps:  17\n",
      "num_trajectories: 31 success rate: 0.96875 Reward: 6.364424431145807\n",
      "num_timesteps:  16\n",
      "num_trajectories: 32 success rate: 0.9696969696969697 Reward: 7.277549849868909\n",
      "num_timesteps:  15\n",
      "num_trajectories: 33 success rate: 0.9705882352941176 Reward: 8.19154401440478\n",
      "num_timesteps:  17\n",
      "num_trajectories: 34 success rate: 0.9714285714285714 Reward: 6.364424431145807\n",
      "num_timesteps:  19\n",
      "num_trajectories: 35 success rate: 0.9722222222222222 Reward: 4.540815020617316\n",
      "num_timesteps:  16\n",
      "num_trajectories: 36 success rate: 0.972972972972973 Reward: 7.277549849868909\n",
      "num_timesteps:  16\n",
      "num_trajectories: 37 success rate: 0.9736842105263158 Reward: 7.277549849868909\n",
      "num_timesteps:  14\n",
      "num_trajectories: 38 success rate: 0.9743589743589743 Reward: 9.106398237295291\n",
      "num_timesteps:  18\n",
      "num_trajectories: 39 success rate: 0.975 Reward: 5.452176533445703\n",
      "num_timesteps:  15\n",
      "num_trajectories: 40 success rate: 0.975609756097561 Reward: 8.19154401440478\n",
      "num_timesteps:  16\n",
      "num_trajectories: 41 success rate: 0.9761904761904762 Reward: 7.277549849868909\n",
      "num_timesteps:  18\n",
      "num_trajectories: 42 success rate: 0.9767441860465116 Reward: 5.452176533445703\n",
      "num_timesteps:  14\n",
      "num_trajectories: 43 success rate: 0.9772727272727273 Reward: 9.106398237295291\n",
      "num_timesteps:  14\n",
      "num_trajectories: 44 success rate: 0.9777777777777777 Reward: 9.106398237295291\n",
      "num_timesteps:  14\n",
      "num_trajectories: 45 success rate: 0.9782608695652174 Reward: 9.106398237295291\n",
      "num_timesteps:  14\n",
      "num_trajectories: 46 success rate: 0.9787234042553191 Reward: 9.106398237295291\n",
      "num_timesteps:  16\n",
      "num_trajectories: 47 success rate: 0.9791666666666666 Reward: 7.6485305937908645\n",
      "num_timesteps:  20\n",
      "num_trajectories: 48 success rate: 0.9795918367346939 Reward: 3.630348846043188\n",
      "num_timesteps:  16\n",
      "num_trajectories: 49 success rate: 0.98 Reward: 7.277549849868909\n",
      "num_timesteps:  15\n",
      "num_trajectories: 50 success rate: 0.9803921568627451 Reward: 6.366170697981574\n",
      "success rate: 0.9803921568627451\n"
     ]
    }
   ],
   "source": [
    "collect_data(env, model, \"grasp\", \"grasp_success_target\", 50, 30)\n",
    "#model.save_replay_buffer(f\"data/td3_expert_grasp_test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roboverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
