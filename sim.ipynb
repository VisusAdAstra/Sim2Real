{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import roboverse\n",
    "env = roboverse.make('Widow250PickPlace-v1', observation_mode=\"pixels\", gui=True)\n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    ext_obs, reward, done, infos = env.step(env.action_space.sample())\n",
    "    print(ext_obs[\"state\"][:3])\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/scripted_collect.py -n 100 -t 30 -e Widow250PickPlace-v0 -pl grasp -a grasp_success_target --noise=0.1 --gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/scripted_collect.py -n 100 -t 30 -e Widow250PickPlace-v0 -pl pickplace -a place_success_target --noise=0.1 --gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/scripted_collect.py -n 100 -t 30 -e Widow250PickPlaceMultiObject-v0 -pl pickplace -a place_success_target --noise=0.1 --gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "\n",
    "import roboverse\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
    "env = roboverse.make(\"Widow250PickPlaceMultiObject-v0\",\n",
    "                         gui=True,\n",
    "                         transpose_image=False)\n",
    "env.reset()\n",
    "\n",
    "# The noise objects for TD3\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "model = TD3(\"MultiInputPolicy\", env, buffer_size=100000, action_noise=action_noise, verbose=1, learning_starts=0)\n",
    "for i in range(500):\n",
    "    print(i)\n",
    "    action = env.action_space.sample()\n",
    "    next_obs, reward, done, infos = env.step(action)\n",
    "    model.replay_buffer.add(next_obs, next_obs, action, reward, done, [{}])\n",
    "model.learn(total_timesteps=10000, log_interval=10)\n",
    "model.save(\"data/td3\")\n",
    "vec_env = model.get_env()\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = TD3.load(\"data/td3\")\n",
    "\n",
    "obs = vec_env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Oct 14 2023 15:44:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argv[0]=--background_color_red=0.8745098114013672\n",
      "argv[1]=--background_color_green=0.21176470816135406\n",
      "argv[2]=--background_color_blue=0.1764705926179886\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=5\n",
      "argv[0] = --unused\n",
      "argv[1] = --background_color_red=0.8745098114013672\n",
      "argv[2] = --background_color_green=0.21176470816135406\n",
      "argv[3] = --background_color_blue=0.1764705926179886\n",
      "argv[4] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Microsoft Corporation\n",
      "GL_RENDERER=D3D12 (Intel(R) UHD Graphics 630)\n",
      "GL_VERSION=4.1 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "GL_SHADING_LANGUAGE_VERSION=4.10\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.1 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "Vendor = Microsoft Corporation\n",
      "Renderer = D3D12 (Intel(R) UHD Graphics 630)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = Microsoft Corporation\n",
      "-1.0\n",
      "{'observation': array([ 7.60743245e-02, -2.21654419e-02,  1.87828466e-01,  2.29194492e-01,\n",
      "       -9.26089764e-01, -1.04939342e+00,  9.67270886e-09, -1.01348259e-01,\n",
      "       -1.96317174e-02,  1.99895296e-02,  4.48686751e-06, -3.08392118e-05,\n",
      "       -3.87872387e-06, -5.73867237e-06,  1.09538505e-05, -5.08076300e-06,\n",
      "        5.10964995e-08, -2.53998151e-04, -9.67000524e-05], dtype=float32), 'achieved_goal': array([-0.10134826, -0.01963172,  0.01998953], dtype=float32), 'desired_goal': array([ 0.07705998, -0.00132606,  0.02      ], dtype=float32)}\n",
      "ven = Microsoft Corporation\n",
      "-1.0\n",
      "{'observation': array([ 4.71951924e-02, -6.44199029e-02,  1.44996524e-01, -4.27768856e-01,\n",
      "       -6.36322141e-01, -7.22118437e-01,  7.02738687e-02, -1.01348385e-01,\n",
      "       -1.96312796e-02,  1.99894346e-02,  4.48783976e-06, -3.55768534e-05,\n",
      "       -7.74457658e-06, -1.61770095e-06,  1.09538014e-05, -9.57692350e-07,\n",
      "        9.50089607e-09, -4.78768852e-05, -9.66187727e-05], dtype=float32), 'achieved_goal': array([-0.10134839, -0.01963128,  0.01998943], dtype=float32), 'desired_goal': array([ 0.07705998, -0.00132606,  0.02      ], dtype=float32)}\n",
      "-1.0\n",
      "{'observation': array([ 4.61886115e-02, -8.07066932e-02,  1.27903625e-01, -1.56557076e-02,\n",
      "       -1.10922635e-01, -1.59237757e-01,  7.88172334e-02, -1.01348430e-01,\n",
      "       -1.96308419e-02,  1.99894179e-02,  4.48802530e-06, -3.64698717e-05,\n",
      "       -1.16089559e-05, -8.40921246e-07,  1.09537632e-05, -1.80520971e-07,\n",
      "        1.81036175e-09, -9.02462489e-06, -9.66050575e-05], dtype=float32), 'achieved_goal': array([-0.10134843, -0.01963084,  0.01998942], dtype=float32), 'desired_goal': array([ 0.07705998, -0.00132606,  0.02      ], dtype=float32)}\n",
      "-1.0\n",
      "{'observation': array([ 2.77330633e-02, -1.06102996e-01,  1.11554034e-01, -5.61041534e-01,\n",
      "       -1.14702499e+00, -3.17824185e-01,  9.58232488e-03, -1.01348460e-01,\n",
      "       -1.96304023e-02,  1.99894141e-02,  4.48806259e-06, -3.66382046e-05,\n",
      "       -1.54731206e-05, -6.94490438e-07,  1.09537305e-05, -3.40300481e-08,\n",
      "        3.89150739e-10, -1.70127612e-06, -9.66040316e-05], dtype=float32), 'achieved_goal': array([-0.10134846, -0.0196304 ,  0.01998941], dtype=float32), 'desired_goal': array([ 0.07705998, -0.00132606,  0.02      ], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import panda_gym\n",
    "import time\n",
    "\n",
    "\n",
    "env = gym.make('PandaPickAndPlace-v3', render_mode=\"human\")\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(10000):\n",
    "    action = env.action_space.sample() # random action\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(reward)\n",
    "    print(observation)\n",
    "    time.sleep(1)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pybullet build time: Oct 14 2023 15:44:17\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Microsoft Corporation\n",
      "GL_RENDERER=D3D12 (Intel(R) UHD Graphics 630)\n",
      "GL_VERSION=4.1 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "GL_SHADING_LANGUAGE_VERSION=4.10\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.1 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.2\n",
      "Vendor = Microsoft Corporation\n",
      "Renderer = D3D12 (Intel(R) UHD Graphics 630)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = Microsoft Corporation\n",
      "ven = Microsoft Corporation\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "reward 0.18530077431141653 info {'ee_pose_success': False, 'euclidean_distance': 0.16857749670086927, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.1897534649208287 info {'ee_pose_success': False, 'euclidean_distance': 0.16620296024207926, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.1942838648069321 info {'ee_pose_success': False, 'euclidean_distance': 0.1638434968728747, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.19889588444233272 info {'ee_pose_success': False, 'euclidean_distance': 0.16149737849283774, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.20358634349760615 info {'ee_pose_success': False, 'euclidean_distance': 0.15916650717145137, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.20835162999552082 info {'ee_pose_success': False, 'euclidean_distance': 0.15685280977916452, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.21318750032310443 info {'ee_pose_success': False, 'euclidean_distance': 0.15455832172876843, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.21825783140117822 info {'ee_pose_success': False, 'euclidean_distance': 0.1522078202197474, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.2234171911066289 info {'ee_pose_success': False, 'euclidean_distance': 0.14987144431741778, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.22864529726049837 info {'ee_pose_success': False, 'euclidean_distance': 0.14755833965048556, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.23393547937100898 info {'ee_pose_success': False, 'euclidean_distance': 0.14527099308284383, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.23927975287933348 info {'ee_pose_success': False, 'euclidean_distance': 0.1430121897428725, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.2446691136466981 info {'ee_pose_success': False, 'euclidean_distance': 0.14078485378049613, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.2500939333078505 info {'ee_pose_success': False, 'euclidean_distance': 0.13859186984585428, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.2555432178631235 info {'ee_pose_success': False, 'euclidean_distance': 0.136436373349344, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.2610045069681923 info {'ee_pose_success': False, 'euclidean_distance': 0.134321760373117, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.2664646806051386 info {'ee_pose_success': False, 'euclidean_distance': 0.13225135747215205, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.2721081941629198 info {'ee_pose_success': False, 'euclidean_distance': 0.1301555519118917, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.2777502067220522 info {'ee_pose_success': False, 'euclidean_distance': 0.12810331061888594, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.28335605933457597 info {'ee_pose_success': False, 'euclidean_distance': 0.12610510119663373, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.05394608814369468 info {'ee_pose_success': False, 'euclidean_distance': 0.29197700988282144, 'target_coord': [0.5, 0.4, -0.3]}\n",
      "reward 0.05484880736200325 info {'ee_pose_success': False, 'euclidean_distance': 0.2903174836125311, 'target_coord': [0.5, 0.4, -0.3]}\n",
      "reward 0.05571598172266237 info {'ee_pose_success': False, 'euclidean_distance': 0.288748824820109, 'target_coord': [0.5, 0.4, -0.3]}\n",
      "reward 0.05654500027905948 info {'ee_pose_success': False, 'euclidean_distance': 0.28727184927236715, 'target_coord': [0.5, 0.4, -0.3]}\n",
      "reward 0.05733210490888315 info {'ee_pose_success': False, 'euclidean_distance': 0.2858894517074186, 'target_coord': [0.5, 0.4, -0.3]}\n",
      "reward 0.058073596016972866 info {'ee_pose_success': False, 'euclidean_distance': 0.284604417596059, 'target_coord': [0.5, 0.4, -0.3]}\n",
      "reward 0.0587658423886032 info {'ee_pose_success': False, 'euclidean_distance': 0.28341945046306133, 'target_coord': [0.5, 0.4, -0.3]}\n",
      "reward 0.05950699602049172 info {'ee_pose_success': False, 'euclidean_distance': 0.2821661393166229, 'target_coord': [0.5, 0.4, -0.3]}\n",
      "reward 0.06020531568387826 info {'ee_pose_success': False, 'euclidean_distance': 0.2809994630168963, 'target_coord': [0.5, 0.4, -0.3]}\n",
      "reward 0.06084834866370773 info {'ee_pose_success': False, 'euclidean_distance': 0.27993705977359384, 'target_coord': [0.5, 0.4, -0.3]}\n",
      "reward 0.061432520561946806 info {'ee_pose_success': False, 'euclidean_distance': 0.27898159331985023, 'target_coord': [0.5, 0.4, -0.3]}\n",
      "reward 0.06195441435036708 info {'ee_pose_success': False, 'euclidean_distance': 0.27813564167811, 'target_coord': [0.5, 0.4, -0.3]}\n",
      "reward 0.06241085355893998 info {'ee_pose_success': False, 'euclidean_distance': 0.2774016083492312, 'target_coord': [0.5, 0.4, -0.3]}\n",
      "reward 0.06279883688231969 info {'ee_pose_success': False, 'euclidean_distance': 0.276781872666167, 'target_coord': [0.5, 0.4, -0.3]}\n",
      "reward 0.06311570154301493 info {'ee_pose_success': False, 'euclidean_distance': 0.27627857045255333, 'target_coord': [0.5, 0.4, -0.3]}\n",
      "reward 0.06335919254531563 info {'ee_pose_success': False, 'euclidean_distance': 0.27589352754758006, 'target_coord': [0.5, 0.4, -0.3]}\n",
      "reward 0.06352727652089073 info {'ee_pose_success': False, 'euclidean_distance': 0.275628591380424, 'target_coord': [0.5, 0.4, -0.3]}\n",
      "reward 0.06373152399022225 info {'ee_pose_success': False, 'euclidean_distance': 0.27530759567106844, 'target_coord': [0.5, 0.4, -0.3]}\n",
      "reward 0.06386766243478607 info {'ee_pose_success': False, 'euclidean_distance': 0.2750942110876167, 'target_coord': [0.5, 0.4, -0.3]}\n",
      "reward 0.06392438934480352 info {'ee_pose_success': False, 'euclidean_distance': 0.2750054310532508, 'target_coord': [0.5, 0.4, -0.3]}\n",
      "reward 0.057909516682608866 info {'ee_pose_success': False, 'euclidean_distance': 0.2848873543788438, 'target_coord': [0.6, -0.1, -0.3]}\n",
      "reward 0.05788446578221339 info {'ee_pose_success': False, 'euclidean_distance': 0.2849306224317306, 'target_coord': [0.6, -0.1, -0.3]}\n",
      "reward 0.057869274239313165 info {'ee_pose_success': False, 'euclidean_distance': 0.2849568704699964, 'target_coord': [0.6, -0.1, -0.3]}\n",
      "reward 0.057864101483616946 info {'ee_pose_success': False, 'euclidean_distance': 0.2849658095606733, 'target_coord': [0.6, -0.1, -0.3]}\n",
      "reward 0.05786911614177573 info {'ee_pose_success': False, 'euclidean_distance': 0.28495714366808894, 'target_coord': [0.6, -0.1, -0.3]}\n",
      "reward 0.05788449098519895 info {'ee_pose_success': False, 'euclidean_distance': 0.2849305788915861, 'target_coord': [0.6, -0.1, -0.3]}\n",
      "reward 0.05791040199424585 info {'ee_pose_success': False, 'euclidean_distance': 0.2848858256061645, 'target_coord': [0.6, -0.1, -0.3]}\n",
      "reward 0.057919641529054086 info {'ee_pose_success': False, 'euclidean_distance': 0.2848698719994027, 'target_coord': [0.6, -0.1, -0.3]}\n",
      "reward 0.05793549363726636 info {'ee_pose_success': False, 'euclidean_distance': 0.28484250660331834, 'target_coord': [0.6, -0.1, -0.3]}\n",
      "reward 0.05796037553161156 info {'ee_pose_success': False, 'euclidean_distance': 0.2847995682399307, 'target_coord': [0.6, -0.1, -0.3]}\n",
      "reward 0.057994488362325895 info {'ee_pose_success': False, 'euclidean_distance': 0.2847407301186947, 'target_coord': [0.6, -0.1, -0.3]}\n",
      "reward 0.05803800179376357 info {'ee_pose_success': False, 'euclidean_distance': 0.28466572796469636, 'target_coord': [0.6, -0.1, -0.3]}\n",
      "reward 0.058091077573740664 info {'ee_pose_success': False, 'euclidean_distance': 0.2845743197076745, 'target_coord': [0.6, -0.1, -0.3]}\n",
      "reward 0.05815398121738356 info {'ee_pose_success': False, 'euclidean_distance': 0.2844660937745692, 'target_coord': [0.6, -0.1, -0.3]}\n",
      "reward 0.05822692755760533 info {'ee_pose_success': False, 'euclidean_distance': 0.28434073584337194, 'target_coord': [0.6, -0.1, -0.3]}\n",
      "reward 0.0583100563539168 info {'ee_pose_success': False, 'euclidean_distance': 0.2841980707294813, 'target_coord': [0.6, -0.1, -0.3]}\n",
      "reward 0.05840365713702757 info {'ee_pose_success': False, 'euclidean_distance': 0.2840376768899268, 'target_coord': [0.6, -0.1, -0.3]}\n",
      "reward 0.05846726270437809 info {'ee_pose_success': False, 'euclidean_distance': 0.2839288293276475, 'target_coord': [0.6, -0.1, -0.3]}\n",
      "reward 0.058536710462597875 info {'ee_pose_success': False, 'euclidean_distance': 0.2838101192283311, 'target_coord': [0.6, -0.1, -0.3]}\n",
      "reward 0.05861560633623411 info {'ee_pose_success': False, 'euclidean_distance': 0.2836754298125017, 'target_coord': [0.6, -0.1, -0.3]}\n",
      "reward 0.18530077431141653 info {'ee_pose_success': False, 'euclidean_distance': 0.16857749670086927, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.1897534649208287 info {'ee_pose_success': False, 'euclidean_distance': 0.16620296024207926, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.1942838648069321 info {'ee_pose_success': False, 'euclidean_distance': 0.1638434968728747, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.19889588444233272 info {'ee_pose_success': False, 'euclidean_distance': 0.16149737849283774, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.20358634349760615 info {'ee_pose_success': False, 'euclidean_distance': 0.15916650717145137, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.20835162999552082 info {'ee_pose_success': False, 'euclidean_distance': 0.15685280977916452, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.21318750032310443 info {'ee_pose_success': False, 'euclidean_distance': 0.15455832172876843, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.21825783140117822 info {'ee_pose_success': False, 'euclidean_distance': 0.1522078202197474, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.2234171911066289 info {'ee_pose_success': False, 'euclidean_distance': 0.14987144431741778, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.22864529726049837 info {'ee_pose_success': False, 'euclidean_distance': 0.14755833965048556, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.23393547937100898 info {'ee_pose_success': False, 'euclidean_distance': 0.14527099308284383, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.23927975287933348 info {'ee_pose_success': False, 'euclidean_distance': 0.1430121897428725, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.2446691136466981 info {'ee_pose_success': False, 'euclidean_distance': 0.14078485378049613, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.2500939333078505 info {'ee_pose_success': False, 'euclidean_distance': 0.13859186984585428, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.2555432178631235 info {'ee_pose_success': False, 'euclidean_distance': 0.136436373349344, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.2610045069681923 info {'ee_pose_success': False, 'euclidean_distance': 0.134321760373117, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.2664646806051386 info {'ee_pose_success': False, 'euclidean_distance': 0.13225135747215205, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.2721081941629198 info {'ee_pose_success': False, 'euclidean_distance': 0.1301555519118917, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.2777502067220522 info {'ee_pose_success': False, 'euclidean_distance': 0.12810331061888594, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.28335605933457597 info {'ee_pose_success': False, 'euclidean_distance': 0.12610510119663373, 'target_coord': [0.5, 0.1, -0.3]}\n",
      "reward 0.07679389584116358 info {'ee_pose_success': False, 'euclidean_distance': 0.25666301232221606, 'target_coord': [0.6, -0.1, -0.2]}\n",
      "reward 0.07657247773125009 info {'ee_pose_success': False, 'euclidean_distance': 0.2569517565346429, 'target_coord': [0.6, -0.1, -0.2]}\n",
      "reward 0.0763662340128813 info {'ee_pose_success': False, 'euclidean_distance': 0.25722146436863574, 'target_coord': [0.6, -0.1, -0.2]}\n",
      "reward 0.0761751607465763 info {'ee_pose_success': False, 'euclidean_distance': 0.2574719843885137, 'target_coord': [0.6, -0.1, -0.2]}\n",
      "reward 0.07599936363304848 info {'ee_pose_success': False, 'euclidean_distance': 0.2577030311980224, 'target_coord': [0.6, -0.1, -0.2]}\n",
      "reward 0.07583895220590071 info {'ee_pose_success': False, 'euclidean_distance': 0.25791432369812756, 'target_coord': [0.6, -0.1, -0.2]}\n",
      "reward 0.07569403931989041 info {'ee_pose_success': False, 'euclidean_distance': 0.25810558624594254, 'target_coord': [0.6, -0.1, -0.2]}\n",
      "reward 0.07548245254857955 info {'ee_pose_success': False, 'euclidean_distance': 0.2583855066315181, 'target_coord': [0.6, -0.1, -0.2]}\n",
      "reward 0.07527722723869074 info {'ee_pose_success': False, 'euclidean_distance': 0.25865776170256105, 'target_coord': [0.6, -0.1, -0.2]}\n",
      "reward 0.0750855294866326 info {'ee_pose_success': False, 'euclidean_distance': 0.2589127422045063, 'target_coord': [0.6, -0.1, -0.2]}\n",
      "reward 0.07490746555789807 info {'ee_pose_success': False, 'euclidean_distance': 0.2591501719755852, 'target_coord': [0.6, -0.1, -0.2]}\n",
      "reward 0.07474312521410108 info {'ee_pose_success': False, 'euclidean_distance': 0.25936980413207944, 'target_coord': [0.6, -0.1, -0.2]}\n",
      "reward 0.07459258523895645 info {'ee_pose_success': False, 'euclidean_distance': 0.25957141702919634, 'target_coord': [0.6, -0.1, -0.2]}\n",
      "reward 0.07445605750559388 info {'ee_pose_success': False, 'euclidean_distance': 0.2597546159755711, 'target_coord': [0.6, -0.1, -0.2]}\n",
      "reward 0.07433368299529669 info {'ee_pose_success': False, 'euclidean_distance': 0.25991910922285905, 'target_coord': [0.6, -0.1, -0.2]}\n",
      "reward 0.07422550122287198 info {'ee_pose_success': False, 'euclidean_distance': 0.260064750560507, 'target_coord': [0.6, -0.1, -0.2]}\n",
      "reward 0.07413174668938378 info {'ee_pose_success': False, 'euclidean_distance': 0.26019114080741196, 'target_coord': [0.6, -0.1, -0.2]}\n",
      "reward 0.07395835509971398 info {'ee_pose_success': False, 'euclidean_distance': 0.2604253113114618, 'target_coord': [0.6, -0.1, -0.2]}\n",
      "reward 0.07379006528131002 info {'ee_pose_success': False, 'euclidean_distance': 0.260653117322178, 'target_coord': [0.6, -0.1, -0.2]}\n",
      "reward 0.07363504485355021 info {'ee_pose_success': False, 'euclidean_distance': 0.260863421367063, 'target_coord': [0.6, -0.1, -0.2]}\n",
      "numActiveThreads = 0\n",
      "stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n",
      "finished\n",
      "numActiveThreads = 0\n",
      "btShutDownExampleBrowser stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n"
     ]
    }
   ],
   "source": [
    "! python roboverse/envs/widow250_eeposition.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import roboverse\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3 import DDPG, HerReplayBuffer\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib.common.wrappers import TimeFeatureWrapper\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "model = TD3()\n",
    "model.replay_buffer.add(observation, next_observation, action, reward, done, [{}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Oct 14 2023 15:44:17\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from roboverse.policies import policies\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "\n",
    "def make_env(env_id: str, rank: int, seed: int = 0):\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "\n",
    "    :param env_id: the environment ID\n",
    "    :param num_env: the number of environments you wish to have in subprocesses\n",
    "    :param seed: the inital seed for RNG\n",
    "    :param rank: index of the subprocess\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = roboverse.make(env_id,\n",
    "                         gui=False,\n",
    "                         observation_mode=\"pixels\",\n",
    "                         transpose_image=False)\n",
    "        #env = TimeFeatureWrapper(env)\n",
    "        #env.reset(seed=seed + rank)\n",
    "        env.reset()\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "\n",
    "def collect_data(env, model, policy, target, num_trajectories=100, num_timesteps=30):\n",
    "    policy_class = policies[policy]\n",
    "    policy = policy_class(env)\n",
    "    num_success = 0\n",
    "    num_saved = 0\n",
    "    accept_trajectory_key = target\n",
    "    noise = 0.1\n",
    "    EPSILON = 0.1\n",
    "\n",
    "    while num_saved < num_trajectories:\n",
    "        num_saved += 1\n",
    "        num_steps = 1e6\n",
    "        rewards = []\n",
    "        env.reset()\n",
    "        policy.reset()\n",
    "        time.sleep(0.1)\n",
    "        for j in range(num_timesteps):\n",
    "            action, agent_info = policy.get_action()\n",
    "\n",
    "            # In case we need to pad actions by 1 for easier realNVP modelling \n",
    "            env_action_dim = env.action_space.shape[0]\n",
    "            #if env_action_dim - action.shape[0] == 1:\n",
    "            #    action = np.append(action, 0)\n",
    "            action += np.random.normal(scale=noise, size=(env_action_dim,))\n",
    "            action = np.clip(action, -1 + EPSILON, 1 - EPSILON)\n",
    "            observation = env.get_observation()\n",
    "            observation[\"image\"] = np.transpose(observation[\"image\"], (2, 0, 1))\n",
    "            next_observation, reward, done, info = env.step(action)\n",
    "            next_observation[\"image\"] = np.transpose(next_observation[\"image\"], (2, 0, 1))\n",
    "            rewards.append(reward)\n",
    "            model.replay_buffer.add(observation, next_observation, action, reward, np.array([done]), [{}])\n",
    "\n",
    "            if info[accept_trajectory_key] and num_steps > 1e3:\n",
    "                num_steps = j\n",
    "\n",
    "            if info[accept_trajectory_key] and j > 23:\n",
    "                break\n",
    "            if done or agent_info['done']:\n",
    "                break\n",
    "\n",
    "        if info[accept_trajectory_key]:\n",
    "            PRINT = False\n",
    "            if PRINT:\n",
    "                print(\"num_timesteps: \", num_steps, rewards)\n",
    "                #print(observation[\"image\"].shape)\n",
    "                #print(next_observation[\"image\"].shape)\n",
    "            num_success += 1\n",
    "        if num_saved%100 == 0:\n",
    "            print(f\"num_trajectories: {num_saved} success rate: {num_success/num_saved} Reward: {sum(rewards)}\")\n",
    "\n",
    "    print(\"success rate: {}\".format(num_success / (num_saved)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import roboverse\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3 import DDPG, HerReplayBuffer\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib.common.wrappers import TimeFeatureWrapper\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "\n",
    "env = roboverse.make(\"Widow250PickPlace-v2\",\n",
    "                         gui=False,\n",
    "                         observation_mode=\"pixels\",\n",
    "                         transpose_image=False)\n",
    "#env = TimeFeatureWrapper(env)\n",
    "#env = DummyVecEnv([make_env(\"Widow250PickPlace-v1\", i) for i in range(4)])\n",
    "obs = env.reset()\n",
    "\n",
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=1000,\n",
    "  save_path=\"./data/tqc/\",\n",
    "  name_prefix=\"tqc_model\",\n",
    "  save_replay_buffer=False,\n",
    "  save_vecnormalize=False,\n",
    ")\n",
    "\n",
    "model = TQC(env=env, batch_size=2048, buffer_size=20000, gamma=0.95, learning_rate=0.001, policy='MultiInputPolicy',\n",
    "             policy_kwargs=dict(net_arch=[512, 512, 512], n_critics=2),\n",
    "             replay_buffer_class=HerReplayBuffer,\n",
    "             replay_buffer_kwargs=dict(goal_selection_strategy='future', n_sampled_goal=4,), #max_episode_length=200,online_sampling=True,\n",
    "             tau=0.05, learning_starts=200, verbose=1)\n",
    "\n",
    "COLLECT=True\n",
    "if COLLECT:\n",
    "    collect_data(env, model, \"pickplace\", \"place_success_target\", 100, 30)\n",
    "    model.save_replay_buffer(f\"data/tqc_expert_pick_place\")\n",
    "else:\n",
    "    print(\"load_replay_buffer\")\n",
    "    model.load_replay_buffer(f\"data/tqc_expert_pick_place\")\n",
    "\n",
    "# print(\"start pre-training from buffer only\")\n",
    "# model.learn(total_timesteps=0, log_interval=5, tb_log_name=\"exp\", reset_num_timesteps = False, progress_bar=True)\n",
    "# model.train(gradient_steps=10000)\n",
    "\n",
    "print(\"start learning\")\n",
    "model.learn(total_timesteps=20000, callback=checkpoint_callback, log_interval=5, tb_log_name=\"exp\", reset_num_timesteps = False, progress_bar=True)\n",
    "model.save(\"data/tqc\")\n",
    "model.save_replay_buffer(f\"data/tqc_expert_pick_place_trained\")\n",
    "\n",
    "print(\"finish learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'dict'>`\u001b[0m\n",
      "  logger.warn(\n",
      "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3c5c82100c477499c9b86a2cc8d7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:219: \n",
       "DeprecationWarning: <span style=\"color: #808000; text-decoration-color: #808000\">WARN: Core environment is written in old step API which returns one bool instead of two. It is </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">recommended to rewrite the environment with new step API. </span>\n",
       "  logger.deprecation(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:219: \n",
       "DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is \u001b[0m\n",
       "\u001b[33mrecommended to rewrite the environment with new step API. \u001b[0m\n",
       "  logger.deprecation(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:225: \n",
       "DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
       "  if not isinstance(done, (bool, np.bool8)):\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:225: \n",
       "DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
       "  if not isinstance(done, (bool, np.bool8)):\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:137: \n",
       "UserWarning: <span style=\"color: #808000; text-decoration-color: #808000\">WARN: The obs returned by the `step()` method was expecting a numpy array, actual type: &lt;class </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">'float'&gt;</span>\n",
       "  logger.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:137: \n",
       "UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting a numpy array, actual type: <class \u001b[0m\n",
       "\u001b[33m'float'>\u001b[0m\n",
       "  logger.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/spaces/box.py:227: UserWarning: <span style=\"color: #808000; text-decoration-color: #808000\">WARN: </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">Casting input x to numpy array.</span>\n",
       "  logger.warn(\"Casting input x to numpy array.\")\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/spaces/box.py:227: UserWarning: \u001b[33mWARN: \u001b[0m\n",
       "\u001b[33mCasting input x to numpy array.\u001b[0m\n",
       "  logger.warn(\"Casting input x to numpy array.\")\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:165: \n",
       "UserWarning: <span style=\"color: #808000; text-decoration-color: #808000\">WARN: The obs returned by the `step()` method is not within the observation space.</span>\n",
       "  logger.warn(f\"{pre} is not within the observation space.\")\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:165: \n",
       "UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
       "  logger.warn(f\"{pre} is not within the observation space.\")\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: \n",
       "UserWarning: <span style=\"color: #808000; text-decoration-color: #808000\">WARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">type: float64</span>\n",
       "  logger.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: \n",
       "UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual \u001b[0m\n",
       "\u001b[33mtype: float64\u001b[0m\n",
       "  logger.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start learning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: <span style=\"color: #808000; text-decoration-color: #808000\">WARN: </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">env.compute_reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">variable you can do `env.unwrapped.compute_reward` for environment variables or </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">`env.get_wrapper_attr('compute_reward')` that will search the reminding wrappers.</span>\n",
       "  logger.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/enhupgu/miniconda3/envs/roboverse/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: \u001b[0m\n",
       "\u001b[33menv.compute_reward to get variables from other wrappers is deprecated and will be removed in v1.0, to get this \u001b[0m\n",
       "\u001b[33mvariable you can do `env.unwrapped.compute_reward` for environment variables or \u001b[0m\n",
       "\u001b[33m`env.get_wrapper_attr('compute_reward')` that will search the reminding wrappers.\u001b[0m\n",
       "  logger.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99       |\n",
      "|    ep_rew_mean     | -99      |\n",
      "| time/              |          |\n",
      "|    episodes        | 5        |\n",
      "|    fps             | 10       |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 495      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -41.7    |\n",
      "|    critic_loss     | 0.487    |\n",
      "|    ent_coef        | 0.746    |\n",
      "|    ent_coef_loss   | -3.95    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 294      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99       |\n",
      "|    ep_rew_mean     | -99      |\n",
      "| time/              |          |\n",
      "|    episodes        | 10       |\n",
      "|    fps             | 8        |\n",
      "|    time_elapsed    | 112      |\n",
      "|    total_timesteps | 990      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -46.6    |\n",
      "|    critic_loss     | 0.408    |\n",
      "|    ent_coef        | 0.455    |\n",
      "|    ent_coef_loss   | -10.6    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 789      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99       |\n",
      "|    ep_rew_mean     | -99      |\n",
      "| time/              |          |\n",
      "|    episodes        | 15       |\n",
      "|    fps             | 8        |\n",
      "|    time_elapsed    | 184      |\n",
      "|    total_timesteps | 1485     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -33.1    |\n",
      "|    critic_loss     | 0.276    |\n",
      "|    ent_coef        | 0.277    |\n",
      "|    ent_coef_loss   | -17.2    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1284     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import roboverse\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3 import DDPG, HerReplayBuffer\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib.common.wrappers import TimeFeatureWrapper\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "\n",
    "\n",
    "env = roboverse.make(\"Widow250PickPlace-v2\",\n",
    "                         gui=False,\n",
    "                         observation_mode=\"pixels\",\n",
    "                         transpose_image=False)\n",
    "#env = TimeFeatureWrapper(env)\n",
    "#env = DummyVecEnv([make_env(\"Widow250PickPlace-v1\", i) for i in range(4)])\n",
    "obs = env.reset()\n",
    "\n",
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=1000,\n",
    "  save_path=\"./data/tqc/\",\n",
    "  name_prefix=\"tqc_model\",\n",
    "  save_replay_buffer=False,\n",
    "  save_vecnormalize=False,\n",
    ")\n",
    "\n",
    "model = TQC(env=env, batch_size=2048, buffer_size=20000, gamma=0.95, learning_rate=0.001, policy='MultiInputPolicy',\n",
    "             policy_kwargs=dict(net_arch=[512, 512, 512], n_critics=2),\n",
    "             replay_buffer_class=HerReplayBuffer,\n",
    "             replay_buffer_kwargs=dict(goal_selection_strategy='future', n_sampled_goal=4),\n",
    "             tau=0.05, learning_starts=200, verbose=1)\n",
    "\n",
    "# COLLECT=True\n",
    "# if COLLECT:\n",
    "#     collect_data(env, model, \"grasp\", \"grasp_success_target\", 5000, 30)\n",
    "#     model.save_replay_buffer(f\"data/tqc_expert_grasp\")\n",
    "# else:\n",
    "#     print(\"load_replay_buffer\")\n",
    "#     model.load_replay_buffer(f\"data/tqc_expert_grasp\")\n",
    "\n",
    "# print(\"start pre-training from buffer only\")\n",
    "# model.learn(total_timesteps=0, log_interval=5, tb_log_name=\"exp\", reset_num_timesteps = False, progress_bar=True)\n",
    "# model.train(gradient_steps=10000)\n",
    "\n",
    "print(\"start learning\")\n",
    "model.learn(total_timesteps=20000, callback=checkpoint_callback, log_interval=5, tb_log_name=\"exp\", reset_num_timesteps = False, progress_bar=True)\n",
    "model.save(\"data/tqc\")\n",
    "model.save_replay_buffer(f\"data/tqc_expert_grasp_trained\")\n",
    "\n",
    "print(\"finish learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model # remove to demonstrate saving and loading\n",
    "# model = TQC.load(\"data/tqc\")\n",
    "\n",
    "# start env with gui\n",
    "env.close()\n",
    "env = roboverse.make(\"Widow250PickPlace-v1\",\n",
    "                         gui=True,\n",
    "                         observation_mode=\"pixels\",\n",
    "                         transpose_image=False)\n",
    "model.set_env(env)\n",
    "env = model.get_env()\n",
    "\n",
    "obs = env.reset()\n",
    "print(\"start render\")\n",
    "for i in range(int(1e4)):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render(\"human\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = OrderedDict([('batch_size', 2048),\n",
    "             ('buffer_size', 1000000),\n",
    "             #('env_wrapper', 'sb3_contrib.common.wrappers.TimeFeatureWrapper'),\n",
    "             ('gamma', 0.95),\n",
    "             ('learning_rate', 0.001),\n",
    "             #('n_timesteps', 1000000.0),\n",
    "             ('policy', 'MultiInputPolicy'),\n",
    "             ('policy_kwargs', 'dict(net_arch=[512, 512, 512], n_critics=2)'),\n",
    "             ('replay_buffer_class', 'HerReplayBuffer'),\n",
    "             ('replay_buffer_kwargs',\n",
    "              \"dict( online_sampling=True, goal_selection_strategy='future', \"\n",
    "              'n_sampled_goal=4, )'),\n",
    "             ('tau', 0.05),\n",
    "             #('normalize', False)\n",
    "             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argv[0]=--background_color_red=0.8745098114013672\n",
      "argv[1]=--background_color_green=0.21176470816135406\n",
      "argv[2]=--background_color_blue=0.1764705926179886\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -50      |\n",
      "|    success_rate    | 0        |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 60       |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 200      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.4    |\n",
      "|    critic_loss     | 0.0321   |\n",
      "|    ent_coef        | 0.907    |\n",
      "|    ent_coef_loss   | -0.657   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 99       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -50      |\n",
      "|    success_rate    | 0        |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 44       |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 400      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.9    |\n",
      "|    critic_loss     | 0.0316   |\n",
      "|    ent_coef        | 0.742    |\n",
      "|    ent_coef_loss   | -2       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 299      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -50      |\n",
      "|    success_rate    | 0        |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 41       |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 600      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 0.246    |\n",
      "|    ent_coef        | 0.608    |\n",
      "|    ent_coef_loss   | -3.31    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 499      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -50      |\n",
      "|    success_rate    | 0        |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 39       |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 800      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23.8    |\n",
      "|    critic_loss     | 0.0745   |\n",
      "|    ent_coef        | 0.499    |\n",
      "|    ent_coef_loss   | -4.56    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 699      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -50      |\n",
      "|    success_rate    | 0        |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 1000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.7    |\n",
      "|    critic_loss     | 0.0771   |\n",
      "|    ent_coef        | 0.41     |\n",
      "|    ent_coef_loss   | -5.73    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 899      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -50      |\n",
      "|    success_rate    | 0        |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 1200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.1    |\n",
      "|    critic_loss     | 0.0791   |\n",
      "|    ent_coef        | 0.338    |\n",
      "|    ent_coef_loss   | -6.88    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1099     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -50      |\n",
      "|    success_rate    | 0        |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 37       |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 1400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17      |\n",
      "|    critic_loss     | 0.0866   |\n",
      "|    ent_coef        | 0.279    |\n",
      "|    ent_coef_loss   | -7.69    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1299     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -50      |\n",
      "|    success_rate    | 0        |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 37       |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 1600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16      |\n",
      "|    critic_loss     | 0.141    |\n",
      "|    ent_coef        | 0.232    |\n",
      "|    ent_coef_loss   | -8.26    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1499     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -50      |\n",
      "|    success_rate    | 0        |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 36       |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 1800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13      |\n",
      "|    critic_loss     | 0.0905   |\n",
      "|    ent_coef        | 0.194    |\n",
      "|    ent_coef_loss   | -9.14    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1699     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | -50      |\n",
      "|    success_rate    | 0        |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 36       |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 2000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11      |\n",
      "|    critic_loss     | 0.0842   |\n",
      "|    ent_coef        | 0.162    |\n",
      "|    ent_coef_loss   | -9.92    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 1899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.9     |\n",
      "|    ep_rew_mean     | -48.9    |\n",
      "|    success_rate    | 0.0227   |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 60       |\n",
      "|    total_timesteps | 2151     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.16    |\n",
      "|    critic_loss     | 0.0853   |\n",
      "|    ent_coef        | 0.141    |\n",
      "|    ent_coef_loss   | -10.7    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2050     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49       |\n",
      "|    ep_rew_mean     | -49      |\n",
      "|    success_rate    | 0.0208   |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 66       |\n",
      "|    total_timesteps | 2351     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.15    |\n",
      "|    critic_loss     | 0.0658   |\n",
      "|    ent_coef        | 0.117    |\n",
      "|    ent_coef_loss   | -11.9    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2250     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.1     |\n",
      "|    ep_rew_mean     | -49      |\n",
      "|    success_rate    | 0.0192   |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 35       |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 2551     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.36    |\n",
      "|    critic_loss     | 0.0523   |\n",
      "|    ent_coef        | 0.0972   |\n",
      "|    ent_coef_loss   | -12.6    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2450     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.1     |\n",
      "|    ep_rew_mean     | -49.1    |\n",
      "|    success_rate    | 0.0179   |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 79       |\n",
      "|    total_timesteps | 2751     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.87    |\n",
      "|    critic_loss     | 0.0416   |\n",
      "|    ent_coef        | 0.0807   |\n",
      "|    ent_coef_loss   | -13.3    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2650     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.2     |\n",
      "|    ep_rew_mean     | -49.2    |\n",
      "|    success_rate    | 0.0167   |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 86       |\n",
      "|    total_timesteps | 2951     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.79    |\n",
      "|    critic_loss     | 0.0358   |\n",
      "|    ent_coef        | 0.0672   |\n",
      "|    ent_coef_loss   | -13.7    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2850     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.2     |\n",
      "|    ep_rew_mean     | -49.2    |\n",
      "|    success_rate    | 0.0156   |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 93       |\n",
      "|    total_timesteps | 3151     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.78    |\n",
      "|    critic_loss     | 0.0362   |\n",
      "|    ent_coef        | 0.056    |\n",
      "|    ent_coef_loss   | -14.4    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3050     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.3     |\n",
      "|    ep_rew_mean     | -49.3    |\n",
      "|    success_rate    | 0.0147   |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 99       |\n",
      "|    total_timesteps | 3351     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.26    |\n",
      "|    critic_loss     | 0.037    |\n",
      "|    ent_coef        | 0.0469   |\n",
      "|    ent_coef_loss   | -14.7    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3250     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.3     |\n",
      "|    ep_rew_mean     | -49.3    |\n",
      "|    success_rate    | 0.0139   |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 105      |\n",
      "|    total_timesteps | 3551     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.409   |\n",
      "|    critic_loss     | 0.0287   |\n",
      "|    ent_coef        | 0.0394   |\n",
      "|    ent_coef_loss   | -14.6    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3450     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.4     |\n",
      "|    ep_rew_mean     | -49.3    |\n",
      "|    success_rate    | 0.0132   |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 111      |\n",
      "|    total_timesteps | 3751     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.0988   |\n",
      "|    critic_loss     | 0.0259   |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -15.3    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3650     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.4     |\n",
      "|    ep_rew_mean     | -49.4    |\n",
      "|    success_rate    | 0.0125   |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 118      |\n",
      "|    total_timesteps | 3951     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.555    |\n",
      "|    critic_loss     | 0.0208   |\n",
      "|    ent_coef        | 0.0279   |\n",
      "|    ent_coef_loss   | -15      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3850     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.4     |\n",
      "|    ep_rew_mean     | -49.4    |\n",
      "|    success_rate    | 0.0119   |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 124      |\n",
      "|    total_timesteps | 4151     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.956    |\n",
      "|    critic_loss     | 0.0192   |\n",
      "|    ent_coef        | 0.0237   |\n",
      "|    ent_coef_loss   | -14.4    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4050     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.4     |\n",
      "|    ep_rew_mean     | -49.4    |\n",
      "|    success_rate    | 0.0114   |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 130      |\n",
      "|    total_timesteps | 4351     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.998    |\n",
      "|    critic_loss     | 0.0205   |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4250     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.5     |\n",
      "|    ep_rew_mean     | -49.5    |\n",
      "|    success_rate    | 0.0109   |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 136      |\n",
      "|    total_timesteps | 4551     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.11     |\n",
      "|    critic_loss     | 0.0172   |\n",
      "|    ent_coef        | 0.0171   |\n",
      "|    ent_coef_loss   | -14.6    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4450     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.5     |\n",
      "|    ep_rew_mean     | -49.5    |\n",
      "|    success_rate    | 0.0104   |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 142      |\n",
      "|    total_timesteps | 4751     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.47     |\n",
      "|    critic_loss     | 0.0153   |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | -15.1    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4650     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.5     |\n",
      "|    ep_rew_mean     | -49.5    |\n",
      "|    success_rate    | 0.01     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 148      |\n",
      "|    total_timesteps | 4951     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.62     |\n",
      "|    critic_loss     | 0.0153   |\n",
      "|    ent_coef        | 0.0124   |\n",
      "|    ent_coef_loss   | -13.9    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4850     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.5     |\n",
      "|    ep_rew_mean     | -49.5    |\n",
      "|    success_rate    | 0.01     |\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 154      |\n",
      "|    total_timesteps | 5151     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.68     |\n",
      "|    critic_loss     | 0.0121   |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | -14.9    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5050     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.5     |\n",
      "|    ep_rew_mean     | -49.5    |\n",
      "|    success_rate    | 0.01     |\n",
      "| time/              |          |\n",
      "|    episodes        | 108      |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 160      |\n",
      "|    total_timesteps | 5351     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.78     |\n",
      "|    critic_loss     | 0.0104   |\n",
      "|    ent_coef        | 0.00893  |\n",
      "|    ent_coef_loss   | -15      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5250     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.5     |\n",
      "|    ep_rew_mean     | -49.5    |\n",
      "|    success_rate    | 0.01     |\n",
      "| time/              |          |\n",
      "|    episodes        | 112      |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 166      |\n",
      "|    total_timesteps | 5551     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.91     |\n",
      "|    critic_loss     | 0.0108   |\n",
      "|    ent_coef        | 0.00758  |\n",
      "|    ent_coef_loss   | -15.3    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5450     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.5     |\n",
      "|    ep_rew_mean     | -49.5    |\n",
      "|    success_rate    | 0.01     |\n",
      "| time/              |          |\n",
      "|    episodes        | 116      |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 172      |\n",
      "|    total_timesteps | 5751     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.91     |\n",
      "|    critic_loss     | 0.00851  |\n",
      "|    ent_coef        | 0.00644  |\n",
      "|    ent_coef_loss   | -14.1    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5650     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49       |\n",
      "|    ep_rew_mean     | -49      |\n",
      "|    success_rate    | 0.02     |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 177      |\n",
      "|    total_timesteps | 5902     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.98     |\n",
      "|    critic_loss     | 0.00856  |\n",
      "|    ent_coef        | 0.0057   |\n",
      "|    ent_coef_loss   | -15.7    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5801     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49       |\n",
      "|    ep_rew_mean     | -49      |\n",
      "|    success_rate    | 0.02     |\n",
      "| time/              |          |\n",
      "|    episodes        | 124      |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 183      |\n",
      "|    total_timesteps | 6102     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2        |\n",
      "|    critic_loss     | 0.00863  |\n",
      "|    ent_coef        | 0.00486  |\n",
      "|    ent_coef_loss   | -15      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6001     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49       |\n",
      "|    ep_rew_mean     | -49      |\n",
      "|    success_rate    | 0.02     |\n",
      "| time/              |          |\n",
      "|    episodes        | 128      |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 189      |\n",
      "|    total_timesteps | 6302     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2        |\n",
      "|    critic_loss     | 0.00874  |\n",
      "|    ent_coef        | 0.00414  |\n",
      "|    ent_coef_loss   | -14.2    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6201     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49       |\n",
      "|    ep_rew_mean     | -49      |\n",
      "|    success_rate    | 0.02     |\n",
      "| time/              |          |\n",
      "|    episodes        | 132      |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 195      |\n",
      "|    total_timesteps | 6502     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2        |\n",
      "|    critic_loss     | 0.00872  |\n",
      "|    ent_coef        | 0.00355  |\n",
      "|    ent_coef_loss   | -13.1    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6401     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49       |\n",
      "|    ep_rew_mean     | -49      |\n",
      "|    success_rate    | 0.02     |\n",
      "| time/              |          |\n",
      "|    episodes        | 136      |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 201      |\n",
      "|    total_timesteps | 6702     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.06     |\n",
      "|    critic_loss     | 0.0104   |\n",
      "|    ent_coef        | 0.00309  |\n",
      "|    ent_coef_loss   | -11      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6601     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49       |\n",
      "|    ep_rew_mean     | -49      |\n",
      "|    success_rate    | 0.02     |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 207      |\n",
      "|    total_timesteps | 6902     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.14     |\n",
      "|    critic_loss     | 0.0125   |\n",
      "|    ent_coef        | 0.00275  |\n",
      "|    ent_coef_loss   | -8.55    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6801     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.5     |\n",
      "|    ep_rew_mean     | -49.5    |\n",
      "|    success_rate    | 0.01     |\n",
      "| time/              |          |\n",
      "|    episodes        | 144      |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 214      |\n",
      "|    total_timesteps | 7102     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.05     |\n",
      "|    critic_loss     | 0.0115   |\n",
      "|    ent_coef        | 0.0025   |\n",
      "|    ent_coef_loss   | -4.88    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7001     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.5     |\n",
      "|    ep_rew_mean     | -49.5    |\n",
      "|    success_rate    | 0.01     |\n",
      "| time/              |          |\n",
      "|    episodes        | 148      |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 220      |\n",
      "|    total_timesteps | 7302     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.15     |\n",
      "|    critic_loss     | 0.0108   |\n",
      "|    ent_coef        | 0.00232  |\n",
      "|    ent_coef_loss   | -3.39    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7201     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.5     |\n",
      "|    ep_rew_mean     | -49.5    |\n",
      "|    success_rate    | 0.01     |\n",
      "| time/              |          |\n",
      "|    episodes        | 152      |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 226      |\n",
      "|    total_timesteps | 7502     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.01     |\n",
      "|    critic_loss     | 0.0101   |\n",
      "|    ent_coef        | 0.00223  |\n",
      "|    ent_coef_loss   | -1.14    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7401     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.5     |\n",
      "|    ep_rew_mean     | -49.5    |\n",
      "|    success_rate    | 0.01     |\n",
      "| time/              |          |\n",
      "|    episodes        | 156      |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 232      |\n",
      "|    total_timesteps | 7702     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.05     |\n",
      "|    critic_loss     | 0.00991  |\n",
      "|    ent_coef        | 0.00217  |\n",
      "|    ent_coef_loss   | -0.902   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7601     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.5     |\n",
      "|    ep_rew_mean     | -49.5    |\n",
      "|    success_rate    | 0.01     |\n",
      "| time/              |          |\n",
      "|    episodes        | 160      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 239      |\n",
      "|    total_timesteps | 7902     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.01     |\n",
      "|    critic_loss     | 0.00951  |\n",
      "|    ent_coef        | 0.00218  |\n",
      "|    ent_coef_loss   | -0.0551  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7801     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.5     |\n",
      "|    ep_rew_mean     | -49.5    |\n",
      "|    success_rate    | 0.01     |\n",
      "| time/              |          |\n",
      "|    episodes        | 164      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 245      |\n",
      "|    total_timesteps | 8102     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.06     |\n",
      "|    critic_loss     | 0.00947  |\n",
      "|    ent_coef        | 0.0022   |\n",
      "|    ent_coef_loss   | 0.709    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8001     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.5     |\n",
      "|    ep_rew_mean     | -49.5    |\n",
      "|    success_rate    | 0.01     |\n",
      "| time/              |          |\n",
      "|    episodes        | 168      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 251      |\n",
      "|    total_timesteps | 8302     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.04     |\n",
      "|    critic_loss     | 0.00932  |\n",
      "|    ent_coef        | 0.00222  |\n",
      "|    ent_coef_loss   | 1.54     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8201     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.5     |\n",
      "|    ep_rew_mean     | -49.5    |\n",
      "|    success_rate    | 0.01     |\n",
      "| time/              |          |\n",
      "|    episodes        | 172      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 258      |\n",
      "|    total_timesteps | 8502     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.03     |\n",
      "|    critic_loss     | 0.00902  |\n",
      "|    ent_coef        | 0.00221  |\n",
      "|    ent_coef_loss   | 0.7      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8401     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.5     |\n",
      "|    ep_rew_mean     | -49.5    |\n",
      "|    success_rate    | 0.01     |\n",
      "| time/              |          |\n",
      "|    episodes        | 176      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 264      |\n",
      "|    total_timesteps | 8702     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.07     |\n",
      "|    critic_loss     | 0.00793  |\n",
      "|    ent_coef        | 0.00218  |\n",
      "|    ent_coef_loss   | -0.76    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8601     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.5     |\n",
      "|    ep_rew_mean     | -49.5    |\n",
      "|    success_rate    | 0.01     |\n",
      "| time/              |          |\n",
      "|    episodes        | 180      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 270      |\n",
      "|    total_timesteps | 8902     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.04     |\n",
      "|    critic_loss     | 0.00765  |\n",
      "|    ent_coef        | 0.00214  |\n",
      "|    ent_coef_loss   | -2.12    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8801     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.5     |\n",
      "|    ep_rew_mean     | -49.5    |\n",
      "|    success_rate    | 0.01     |\n",
      "| time/              |          |\n",
      "|    episodes        | 184      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 276      |\n",
      "|    total_timesteps | 9102     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.18     |\n",
      "|    critic_loss     | 0.00844  |\n",
      "|    ent_coef        | 0.00204  |\n",
      "|    ent_coef_loss   | 0.205    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9001     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49.5     |\n",
      "|    ep_rew_mean     | -49.5    |\n",
      "|    success_rate    | 0.01     |\n",
      "| time/              |          |\n",
      "|    episodes        | 188      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 283      |\n",
      "|    total_timesteps | 9302     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.17     |\n",
      "|    critic_loss     | 0.00995  |\n",
      "|    ent_coef        | 0.0019   |\n",
      "|    ent_coef_loss   | -1.31    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9201     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49       |\n",
      "|    ep_rew_mean     | -49      |\n",
      "|    success_rate    | 0.02     |\n",
      "| time/              |          |\n",
      "|    episodes        | 192      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 288      |\n",
      "|    total_timesteps | 9453     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.25     |\n",
      "|    critic_loss     | 0.00825  |\n",
      "|    ent_coef        | 0.0018   |\n",
      "|    ent_coef_loss   | -3.38    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9352     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 49       |\n",
      "|    ep_rew_mean     | -49      |\n",
      "|    success_rate    | 0.02     |\n",
      "| time/              |          |\n",
      "|    episodes        | 196      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 294      |\n",
      "|    total_timesteps | 9653     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.33     |\n",
      "|    critic_loss     | 0.00855  |\n",
      "|    ent_coef        | 0.00161  |\n",
      "|    ent_coef_loss   | -3.98    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9552     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.5     |\n",
      "|    ep_rew_mean     | -48.5    |\n",
      "|    success_rate    | 0.03     |\n",
      "| time/              |          |\n",
      "|    episodes        | 200      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 299      |\n",
      "|    total_timesteps | 9804     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.34     |\n",
      "|    critic_loss     | 0.00926  |\n",
      "|    ent_coef        | 0.00146  |\n",
      "|    ent_coef_loss   | -5.93    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9703     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.5     |\n",
      "|    ep_rew_mean     | -48.5    |\n",
      "|    success_rate    | 0.03     |\n",
      "| time/              |          |\n",
      "|    episodes        | 204      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 305      |\n",
      "|    total_timesteps | 10004    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.37     |\n",
      "|    critic_loss     | 0.00962  |\n",
      "|    ent_coef        | 0.0013   |\n",
      "|    ent_coef_loss   | -1.54    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9903     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.5     |\n",
      "|    ep_rew_mean     | -48.5    |\n",
      "|    success_rate    | 0.03     |\n",
      "| time/              |          |\n",
      "|    episodes        | 208      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 311      |\n",
      "|    total_timesteps | 10204    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.44     |\n",
      "|    critic_loss     | 0.00769  |\n",
      "|    ent_coef        | 0.0012   |\n",
      "|    ent_coef_loss   | 0.854    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 10103    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.5     |\n",
      "|    ep_rew_mean     | -48.5    |\n",
      "|    success_rate    | 0.03     |\n",
      "| time/              |          |\n",
      "|    episodes        | 212      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 317      |\n",
      "|    total_timesteps | 10404    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.54     |\n",
      "|    critic_loss     | 0.00961  |\n",
      "|    ent_coef        | 0.00114  |\n",
      "|    ent_coef_loss   | -0.238   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 10303    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.5     |\n",
      "|    ep_rew_mean     | -48.5    |\n",
      "|    success_rate    | 0.03     |\n",
      "| time/              |          |\n",
      "|    episodes        | 216      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 322      |\n",
      "|    total_timesteps | 10604    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.53     |\n",
      "|    critic_loss     | 0.0101   |\n",
      "|    ent_coef        | 0.00103  |\n",
      "|    ent_coef_loss   | -1.15    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 10503    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.6     |\n",
      "|    ep_rew_mean     | -48.6    |\n",
      "|    success_rate    | 0.03     |\n",
      "| time/              |          |\n",
      "|    episodes        | 220      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 327      |\n",
      "|    total_timesteps | 10764    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.63     |\n",
      "|    critic_loss     | 0.00889  |\n",
      "|    ent_coef        | 0.000983 |\n",
      "|    ent_coef_loss   | -1.43    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 10663    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.1     |\n",
      "|    ep_rew_mean     | -48.1    |\n",
      "|    success_rate    | 0.04     |\n",
      "| time/              |          |\n",
      "|    episodes        | 224      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 332      |\n",
      "|    total_timesteps | 10915    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.66     |\n",
      "|    critic_loss     | 0.00954  |\n",
      "|    ent_coef        | 0.000935 |\n",
      "|    ent_coef_loss   | 1.33     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 10814    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.1     |\n",
      "|    ep_rew_mean     | -48.1    |\n",
      "|    success_rate    | 0.04     |\n",
      "| time/              |          |\n",
      "|    episodes        | 228      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 338      |\n",
      "|    total_timesteps | 11115    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.74     |\n",
      "|    critic_loss     | 0.00989  |\n",
      "|    ent_coef        | 0.000916 |\n",
      "|    ent_coef_loss   | 0.384    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11014    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.1     |\n",
      "|    ep_rew_mean     | -48.1    |\n",
      "|    success_rate    | 0.04     |\n",
      "| time/              |          |\n",
      "|    episodes        | 232      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 344      |\n",
      "|    total_timesteps | 11315    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.75     |\n",
      "|    critic_loss     | 0.0093   |\n",
      "|    ent_coef        | 0.000862 |\n",
      "|    ent_coef_loss   | -1.98    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11214    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.1     |\n",
      "|    ep_rew_mean     | -48.1    |\n",
      "|    success_rate    | 0.04     |\n",
      "| time/              |          |\n",
      "|    episodes        | 236      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 350      |\n",
      "|    total_timesteps | 11515    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.67     |\n",
      "|    critic_loss     | 0.00943  |\n",
      "|    ent_coef        | 0.000858 |\n",
      "|    ent_coef_loss   | 0.12     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11414    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.1     |\n",
      "|    ep_rew_mean     | -48.1    |\n",
      "|    success_rate    | 0.04     |\n",
      "| time/              |          |\n",
      "|    episodes        | 240      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 356      |\n",
      "|    total_timesteps | 11715    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.71     |\n",
      "|    critic_loss     | 0.00936  |\n",
      "|    ent_coef        | 0.000859 |\n",
      "|    ent_coef_loss   | -2.05    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11614    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.1     |\n",
      "|    ep_rew_mean     | -48.1    |\n",
      "|    success_rate    | 0.04     |\n",
      "| time/              |          |\n",
      "|    episodes        | 244      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 362      |\n",
      "|    total_timesteps | 11915    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.7      |\n",
      "|    critic_loss     | 0.00795  |\n",
      "|    ent_coef        | 0.000857 |\n",
      "|    ent_coef_loss   | -0.885   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11814    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.1     |\n",
      "|    ep_rew_mean     | -48.1    |\n",
      "|    success_rate    | 0.04     |\n",
      "| time/              |          |\n",
      "|    episodes        | 248      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 368      |\n",
      "|    total_timesteps | 12115    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.82     |\n",
      "|    critic_loss     | 0.00972  |\n",
      "|    ent_coef        | 0.000925 |\n",
      "|    ent_coef_loss   | 1.27     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12014    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.1     |\n",
      "|    ep_rew_mean     | -48.1    |\n",
      "|    success_rate    | 0.04     |\n",
      "| time/              |          |\n",
      "|    episodes        | 252      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 374      |\n",
      "|    total_timesteps | 12315    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.84     |\n",
      "|    critic_loss     | 0.0111   |\n",
      "|    ent_coef        | 0.00099  |\n",
      "|    ent_coef_loss   | 1.88     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12214    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.1     |\n",
      "|    ep_rew_mean     | -48.1    |\n",
      "|    success_rate    | 0.04     |\n",
      "| time/              |          |\n",
      "|    episodes        | 256      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 380      |\n",
      "|    total_timesteps | 12515    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.85     |\n",
      "|    critic_loss     | 0.0114   |\n",
      "|    ent_coef        | 0.00102  |\n",
      "|    ent_coef_loss   | -2.7     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12414    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.1     |\n",
      "|    ep_rew_mean     | -48.1    |\n",
      "|    success_rate    | 0.04     |\n",
      "| time/              |          |\n",
      "|    episodes        | 260      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 386      |\n",
      "|    total_timesteps | 12715    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.88     |\n",
      "|    critic_loss     | 0.011    |\n",
      "|    ent_coef        | 0.000944 |\n",
      "|    ent_coef_loss   | -3.07    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12614    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.1     |\n",
      "|    ep_rew_mean     | -48.1    |\n",
      "|    success_rate    | 0.04     |\n",
      "| time/              |          |\n",
      "|    episodes        | 264      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 392      |\n",
      "|    total_timesteps | 12915    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.79     |\n",
      "|    critic_loss     | 0.0114   |\n",
      "|    ent_coef        | 0.000873 |\n",
      "|    ent_coef_loss   | 0.303    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12814    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.1     |\n",
      "|    ep_rew_mean     | -48.1    |\n",
      "|    success_rate    | 0.04     |\n",
      "| time/              |          |\n",
      "|    episodes        | 268      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 398      |\n",
      "|    total_timesteps | 13115    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.98     |\n",
      "|    critic_loss     | 0.00922  |\n",
      "|    ent_coef        | 0.000839 |\n",
      "|    ent_coef_loss   | 0.848    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 13014    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 47.6     |\n",
      "|    ep_rew_mean     | -47.6    |\n",
      "|    success_rate    | 0.05     |\n",
      "| time/              |          |\n",
      "|    episodes        | 272      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 402      |\n",
      "|    total_timesteps | 13266    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.89     |\n",
      "|    critic_loss     | 0.00856  |\n",
      "|    ent_coef        | 0.000893 |\n",
      "|    ent_coef_loss   | 0.463    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 13165    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 47.6     |\n",
      "|    ep_rew_mean     | -47.6    |\n",
      "|    success_rate    | 0.05     |\n",
      "| time/              |          |\n",
      "|    episodes        | 276      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 408      |\n",
      "|    total_timesteps | 13466    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.89     |\n",
      "|    critic_loss     | 0.0078   |\n",
      "|    ent_coef        | 0.000925 |\n",
      "|    ent_coef_loss   | -0.523   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 13365    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 47.6     |\n",
      "|    ep_rew_mean     | -47.6    |\n",
      "|    success_rate    | 0.05     |\n",
      "| time/              |          |\n",
      "|    episodes        | 280      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 414      |\n",
      "|    total_timesteps | 13666    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.79     |\n",
      "|    critic_loss     | 0.00837  |\n",
      "|    ent_coef        | 0.000958 |\n",
      "|    ent_coef_loss   | 0.514    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 13565    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 47.6     |\n",
      "|    ep_rew_mean     | -47.6    |\n",
      "|    success_rate    | 0.05     |\n",
      "| time/              |          |\n",
      "|    episodes        | 284      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 420      |\n",
      "|    total_timesteps | 13866    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.86     |\n",
      "|    critic_loss     | 0.00889  |\n",
      "|    ent_coef        | 0.000916 |\n",
      "|    ent_coef_loss   | -0.734   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 13765    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 47.6     |\n",
      "|    ep_rew_mean     | -47.6    |\n",
      "|    success_rate    | 0.05     |\n",
      "| time/              |          |\n",
      "|    episodes        | 288      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 426      |\n",
      "|    total_timesteps | 14066    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.91     |\n",
      "|    critic_loss     | 0.0104   |\n",
      "|    ent_coef        | 0.000883 |\n",
      "|    ent_coef_loss   | 1.55     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 13965    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 47.5     |\n",
      "|    ep_rew_mean     | -47.5    |\n",
      "|    success_rate    | 0.06     |\n",
      "| time/              |          |\n",
      "|    episodes        | 292      |\n",
      "|    fps             | 33       |\n",
      "|    time_elapsed    | 430      |\n",
      "|    total_timesteps | 14205    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.91     |\n",
      "|    critic_loss     | 0.00997  |\n",
      "|    ent_coef        | 0.000882 |\n",
      "|    ent_coef_loss   | -1.04    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 14104    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import panda_gym\n",
    "from stable_baselines3 import DDPG, HerReplayBuffer\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib.common.wrappers import TimeFeatureWrapper\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "env = gym.make(\"PandaPickAndPlace-v3\")\n",
    "env = TimeFeatureWrapper(env)\n",
    "model = TQC(env=env, batch_size=2048, buffer_size=100000, gamma=0.95, learning_rate=0.001, policy='MultiInputPolicy',\n",
    "             policy_kwargs=dict(net_arch=[512, 512, 512], n_critics=2),\n",
    "             replay_buffer_class=HerReplayBuffer,\n",
    "             replay_buffer_kwargs=dict(goal_selection_strategy='future', n_sampled_goal=4),\n",
    "             tau=0.05, log_interval=10, verbose=1)\n",
    "\n",
    "model.learn(1_000_000)\n",
    "model.save('data/pick_place/tqc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roboverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
